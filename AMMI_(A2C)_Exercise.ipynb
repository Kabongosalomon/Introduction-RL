{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AMMI (A2C) Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kabongosalomon/Introduction-RL/blob/master/AMMI_(A2C)_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CWjie7h0zGdD"
      },
      "source": [
        "## Install, import and utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD74OdCFhUN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VrO05StZyFee",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJvy13_UhLUq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e891ca6-a042-44a2-b221-318b42727a19"
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (45.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIQC-DpYhBpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import clear_output\n",
        "from pathlib import Path\n",
        "\n",
        "import random, os.path, math, glob, csv, base64, itertools, sys\n",
        "from pprint import pprint\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import io\n",
        "from IPython.display import HTML\n",
        "\n",
        "from copy import deepcopy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qkmxo4hhwIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2cfa9738-83ec-40fa-e00c-a8009ed3b5c2"
      },
      "source": [
        "# The following code is will be used to visualize the environments.\n",
        "def show_video(directory):\n",
        "    html = []\n",
        "    for mp4 in Path(directory).glob(\"*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append('''<video alt=\"{}\" autoplay \n",
        "                      loop controls style=\"height: 400px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "    \n",
        "def make_seed(seed):\n",
        "    np.random.seed(seed=seed)\n",
        "    torch.manual_seed(seed=seed)\n",
        "  \n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1003'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1003'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I9kdsiBQ31mF"
      },
      "source": [
        "## A2C\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka9t6kxhuczD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "for n = 1, ..., N //number of iterations<br>\n",
        "> collect $K$ samples\n",
        "\n",
        "> for i = 0,...,$K$:\n",
        "\n",
        ">> execute action $a_i\\sim\\pi(s)$\n",
        "\n",
        ">> observe reward $r_i$ and next state $s'_i$\n",
        "\n",
        ">> store $(s_i,a_i,r_i,s'_i)$\n",
        "\n",
        ">> if done: reset\n",
        "\n",
        "> compute target $y_i$ for each sample $i \\in [0, K]$\n",
        "\n",
        "> compute $\\delta_i$ for each sample $i \\in [0,K]$\n",
        "\n",
        "> Compute estimate of $V$ by gradient descent on MSE\n",
        "\n",
        "$$L_{MSE}(\\omega|data) =\\sum_{i=1}^{K} (V_{\\omega}(s_i) - y_i)^2$$\n",
        "\n",
        "> Update policy by gradient descent on policy loss\n",
        "\n",
        "$$L_\\pi(\\theta|data) = \\frac{1}{K} \\sum_{i=1}^K \n",
        "      \\Big[ \n",
        "                                \\log \\pi_\\theta( a_i|s_i) \\delta_i + \\lambda_e \\Omega(\\pi_{\\theta}(\\cdot| s_i))\n",
        "                        \\Big]$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5DlNKriHyFg3",
        "colab": {}
      },
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "     This network represents the policy\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, action_size):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.n_actions = action_size\n",
        "        self.dim_observation = input_size\n",
        "        \n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features=self.dim_observation, out_features=hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_size, out_features=self.n_actions),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        \n",
        "    def policy(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        return self.net(state)\n",
        "    \n",
        "    def sample_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        action = torch.multinomial(self.policy(state), 1)\n",
        "        return action.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SgnRZXEayFhA",
        "colab": {}
      },
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "  \"\"\"\n",
        "   This class represents the value function\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "      super(ValueNetwork, self).__init__()\n",
        "      self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "      self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "      self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = F.relu(self.fc1(x))\n",
        "      out = F.relu(self.fc2(out))\n",
        "      out = self.fc3(out)\n",
        "      return out\n",
        "  \n",
        "  def value(self, state):\n",
        "      state = torch.tensor(state, dtype=torch.float)\n",
        "      return self.forward(state)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUsSnXI9Giv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can select your environment here\n",
        "env_id = 'CartPole-v1'  #@param [\"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\"]\n",
        "env = gym.make(env_id)\n",
        "\n",
        "eval_env = gym.make(env_id) # environment to evaluate the policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLSXeLIeCKjT",
        "colab_type": "text"
      },
      "source": [
        "# Main algorithm\n",
        "\n",
        "for n = 1, ..., N //number of iterations<br>\n",
        "> collect $K$ samples\n",
        "\n",
        "> for i = 0,...,$K$:\n",
        "\n",
        ">> execute action $a_i\\sim\\pi(s)$\n",
        "\n",
        ">> observe reward $r_i$ and next state $s'_i$\n",
        "\n",
        ">> store $(s_i,a_i,r_i,s'_i)$\n",
        "\n",
        ">> if done: reset\n",
        "\n",
        "> compute target $y_i$ for each sample $i \\in [0, K]$ \n",
        "\n",
        "$$y_{t}^{i}=\\sum_{k=t}^{T_{i}} \\gamma^{k-t} r_{k}+\\gamma^{T_{i}-t+1} V_{\\omega}\\left(s_{T_{i}}\\right) \\cdot 1\\left(s_{T_{i}} \\text { is terminal }\\right)$$\n",
        "\n",
        "\n",
        "> compute $\\delta_i$ for each sample $i \\in [0,K]$\n",
        "\n",
        "$$\n",
        "  \\delta_{t}^{i}=y_{t}^{i}-V_{\\omega}\\left(s_{t}^{i}\\right)\n",
        "$$\n",
        "\n",
        "> Compute estimate of $V$ by gradient descent on MSE\n",
        "\n",
        "$$L_{MSE}(\\omega|data) =\\sum_{i=1}^{K} (V_{\\omega}(s_i) - y_i)^2$$\n",
        "\n",
        "> Update policy by gradient descent on policy loss\n",
        "\n",
        "$$L_\\pi(\\theta|data) = \\frac{1}{K} \\sum_{i=1}^K \n",
        "      \\Big[ \n",
        "                                \\log \\pi_\\theta( a_i|s_i) \\delta_i + \\lambda_e \\Omega(\\pi_{\\theta}(\\cdot| s_i))\n",
        "                        \\Big]$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$$Omega(\\pi(.| s))=\\sum_{a} \\pi(a | s) \\log \\pi(a | s)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVN-qhxY7o7W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "e08b6528-5259-4891-9514-33136d322fa1"
      },
      "source": [
        "# Define you networks\n",
        "value_network = ValueNetwork(env.observation_space.shape[0], 16, 1)\n",
        "actor_network = ActorNetwork(env.observation_space.shape[0], 16, env.action_space.n)\n",
        "print(value_network)\n",
        "print(actor_network)\n",
        "\n",
        "# Define your optimizers\n",
        "value_network_optimizer = torch.optim.RMSprop(value_network.parameters(), lr=0.01)\n",
        "actor_network_optimizer = torch.optim.RMSprop(actor_network.parameters(), lr=0.01)\n",
        "\n",
        "objective = nn.MSELoss()\n",
        "\n",
        "\n",
        "num_iterations = 250   # How many update step do we perform   \n",
        "batch_size = 1024      # How many samples to collect\n",
        "gamma = 1\n",
        "lambda_entropy = 0.001 # regularization coefficient for entropy\n",
        "\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "    # Initialize batch storage\n",
        "    batch_losses = torch.zeros(batch_size)\n",
        "    batch_returns = np.zeros(batch_size)\n",
        "\n",
        "\n",
        "    states = np.empty((batch_size,) + env.observation_space.shape, dtype=np.float)        # shape (batch_size, state_dim)\n",
        "    rewards = np.empty((batch_size,), dtype=np.float)                                     # shape (batch_size, )                                 \n",
        "    next_states = np.empty((batch_size,) + env.observation_space.shape, dtype=np.float)   # shape (batch_size, state_dim)\n",
        "    dones = np.empty((batch_size,), dtype=np.bool)                                        # shape (batch_size, ) \n",
        "    proba = torch.empty((batch_size,), dtype=np.float)                                    # shape (batch_size, ), store pi(a_t|s_t)\n",
        "    next_value = 0                                \n",
        "  \n",
        "    # Intialize environment\n",
        "    state = env.reset()\n",
        "\n",
        "    # Generate batch\n",
        "    for i in range(batch_size):\n",
        "        action = actor_network.sample_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        states[i] = state\n",
        "        rewards[i] = reward\n",
        "        next_states[i] = next_state\n",
        "        dones[i] = done\n",
        "        proba[i] = actor_network.policy(state)[action]\n",
        "\n",
        "        state = next_state\n",
        "        if done:\n",
        "          state = env.reset()\n",
        "\n",
        "    if not done:\n",
        "        next_value = value_network.value(next_states[-1]).detach().numpy()[0]\n",
        "\n",
        "    # compute returns (with bootstrapping)\n",
        "    T = len(rewards)\n",
        "    returns = np.zeros((T+1,), dtype=np.float)\n",
        "    returns[-1] = next_value\n",
        "\n",
        "    for t in reversed(range(T)):\n",
        "        returns[t] = rewards[t] + gamma * returns[t+1]\n",
        "\n",
        "        if dones[t]:\n",
        "            returns[t] = rewards[t]\n",
        "\n",
        "\n",
        "    # compute advantage\n",
        "    values = value_network.value(states)\n",
        "    advantages = returns[0:T] - values.detach().numpy().squeeze()\n",
        "\n",
        "    # Compute MSE\n",
        "    value_network_optimizer.zero_grad()\n",
        "    target = torch.tensor(returns[0:T].reshape(-1, 1), dtype=torch.float)\n",
        "    loss_value =  objective(values, target)                          \n",
        "    loss_value.backward()\n",
        "    value_network_optimizer.step()\n",
        "\n",
        "    # compute entropy term\n",
        "    # cross_entropy_term = - (actor_network.policy(states) * \\\n",
        "    #                         torch.log(actor_network.policy(states))).sum()\n",
        "\n",
        "    dist = actor_network.policy(states)\n",
        "    cross_entropy_term = - torch.sum(dist * torch.log(dist))\n",
        "    \n",
        "\n",
        "    # Compute Actor Gradient\n",
        "    \n",
        "    loss_policy = torch.zeros(1)\n",
        "    for t in range(T):\n",
        "        loss_policy += torch.log(proba[t]) * advantages[t]\n",
        "\n",
        "    loss_policy = -1 * loss_policy\n",
        "\n",
        "    actor_network_optimizer.zero_grad()\n",
        "    loss_policy = loss_policy - lambda_entropy * cross_entropy_term\n",
        "    loss_policy.backward()\n",
        "    actor_network_optimizer.step()\n",
        "\n",
        "    # this code is to evaluate the policy every 10 iterations\n",
        "    if( (iteration+1)%10 == 0 ):\n",
        "        eval_rewards = np.zeros(5)\n",
        "        for sim in range(5):\n",
        "            eval_done = False\n",
        "            eval_state = eval_env.reset()\n",
        "            while not eval_done:\n",
        "                eval_action = actor_network.sample_action(eval_state)\n",
        "                eval_next_state, eval_reward, eval_done, _ = eval_env.step(eval_action)\n",
        "                eval_rewards[sim] += eval_reward\n",
        "                eval_state = eval_next_state\n",
        "        \n",
        "        print(\"it, rewards = \", \n",
        "              iteration +1, eval_rewards.mean())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ValueNetwork(\n",
            "  (fc1): Linear(in_features=4, out_features=16, bias=True)\n",
            "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
            "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
            ")\n",
            "ActorNetwork(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=16, out_features=2, bias=True)\n",
            "    (5): Softmax(dim=-1)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "it, rewards =  10 179.4\n",
            "it, rewards =  20 112.2\n",
            "it, rewards =  30 435.4\n",
            "it, rewards =  40 500.0\n",
            "it, rewards =  50 500.0\n",
            "it, rewards =  60 500.0\n",
            "it, rewards =  70 468.2\n",
            "it, rewards =  80 500.0\n",
            "it, rewards =  90 500.0\n",
            "it, rewards =  100 377.2\n",
            "it, rewards =  110 500.0\n",
            "it, rewards =  120 500.0\n",
            "it, rewards =  130 500.0\n",
            "it, rewards =  140 500.0\n",
            "it, rewards =  150 500.0\n",
            "it, rewards =  160 202.2\n",
            "it, rewards =  170 113.2\n",
            "it, rewards =  180 125.8\n",
            "it, rewards =  190 195.0\n",
            "it, rewards =  200 204.8\n",
            "it, rewards =  210 500.0\n",
            "it, rewards =  220 500.0\n",
            "it, rewards =  230 500.0\n",
            "it, rewards =  240 500.0\n",
            "it, rewards =  250 500.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0HW0jgTxdn8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "73a1f1eb-299e-4a50-f5a7-fe7bfccb08aa"
      },
      "source": [
        "env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)\n",
        "for episode in range(4):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    while not done:\n",
        "        action = actor_network.sample_action(state)\n",
        "        state, reward, done, info = env.step(action)\n",
        "env.close()\n",
        "show_video(\"./gym-results\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-4a196cd281c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         )\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mrender_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ansi'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mansi_mode\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     raise ImportError('''\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mcompat_platform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'darwin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcocoa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoaConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m  \u001b[0;31m# noqa: F821\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;31m# XXX remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUG39kDbf_6O",
        "colab_type": "text"
      },
      "source": [
        "<h1>About the Authors:</h1> \n",
        "\n",
        "<a href=\"https://skabongo.github.io/\">Salomon Kabongo</a>, Master degree student at <a href=\"https://aims.ac.za/\">the African Master in Machine Intelligence (AMMI, Ghana)</a> his research focused on the use machine learning technique in the field of Natural Language Processing, learn more about him [here](https://skabongo.github.io/) or https://skabongo.github.io/.\n",
        "\n",
        "**References :** *Introduction to Reiforcement learning AMMI, course* by [Matteo Pirotta](https://teopir.github.io/), Research Scientist at Facebook AI Research, assisted by [Floian Strub](https://fstrub95.github.io/) and [Omar Darwiche Domingue](https://omardrwch.github.io/).\n",
        "\n",
        "Copyright &copy; 2020. This notebook and its source code are released under the terms of the <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">Apache License 2.0</a>."
      ]
    }
  ]
}