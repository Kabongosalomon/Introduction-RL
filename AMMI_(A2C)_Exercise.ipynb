{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AMMI (A2C) Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CWjie7h0zGdD"
      },
      "source": [
        "## Install, import and utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD74OdCFhUN_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VrO05StZyFee",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJvy13_UhLUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIQC-DpYhBpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import clear_output\n",
        "from pathlib import Path\n",
        "\n",
        "import random, os.path, math, glob, csv, base64, itertools, sys\n",
        "from pprint import pprint\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import io\n",
        "from IPython.display import HTML\n",
        "\n",
        "from copy import deepcopy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qkmxo4hhwIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The following code is will be used to visualize the environments.\n",
        "def show_video(directory):\n",
        "    html = []\n",
        "    for mp4 in Path(directory).glob(\"*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append('''<video alt=\"{}\" autoplay \n",
        "                      loop controls style=\"height: 400px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "    \n",
        "def make_seed(seed):\n",
        "    np.random.seed(seed=seed)\n",
        "    torch.manual_seed(seed=seed)\n",
        "  \n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I9kdsiBQ31mF"
      },
      "source": [
        "## A2C\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka9t6kxhuczD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "for n = 1, ..., N //number of iterations<br>\n",
        "> collect $K$ samples\n",
        "\n",
        "> for i = 0,...,$K$:\n",
        "\n",
        ">> execute action $a_i\\sim\\pi(s)$\n",
        "\n",
        ">> observe reward $r_i$ and next state $s'_i$\n",
        "\n",
        ">> store $(s_i,a_i,r_i,s'_i)$\n",
        "\n",
        ">> if done: reset\n",
        "\n",
        "> compute target $y_i$ for each sample $i \\in [0, K]$\n",
        "\n",
        "> compute $\\delta_i$ for each sample $i \\in [0,K]$\n",
        "\n",
        "> Compute estimate of $V$ by gradient descent on MSE\n",
        "\n",
        "$$L_{MSE}(\\omega|data) =\\sum_{i=1}^{K} (V_{\\omega}(s_i) - y_i)^2$$\n",
        "\n",
        "> Update policy by gradient descent on policy loss\n",
        "\n",
        "$$L_\\pi(\\theta|data) = \\frac{1}{K} \\sum_{i=1}^K \n",
        "      \\Big[ \n",
        "                                \\log \\pi_\\theta( a_i|s_i) \\delta_i + \\lambda_e \\Omega(\\pi_{\\theta}(\\cdot| s_i))\n",
        "                        \\Big]$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5DlNKriHyFg3",
        "colab": {}
      },
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "     This network represents the policy\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, action_size):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.n_actions = action_size\n",
        "        self.dim_observation = input_size\n",
        "        \n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features=self.dim_observation, out_features=hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_size, out_features=hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_size, out_features=self.n_actions),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        \n",
        "    def policy(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        return self.net(state)\n",
        "    \n",
        "    def sample_action(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float)\n",
        "        action = torch.multinomial(self.policy(state), 1)\n",
        "        return action.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SgnRZXEayFhA",
        "colab": {}
      },
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "  \"\"\"\n",
        "   This class represents the value function\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "      super(ValueNetwork, self).__init__()\n",
        "      self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "      self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "      self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = F.relu(self.fc1(x))\n",
        "      out = F.relu(self.fc2(out))\n",
        "      out = self.fc3(out)\n",
        "      return out\n",
        "  \n",
        "  def value(self, state):\n",
        "      state = torch.tensor(state, dtype=torch.float)\n",
        "      return self.forward(state)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUsSnXI9Giv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can select your environment here\n",
        "env_id = 'CartPole-v1'  #@param [\"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\"]\n",
        "env = gym.make(env_id)\n",
        "\n",
        "eval_env = gym.make(env_id) # environment to evaluate the policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVN-qhxY7o7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define you networks\n",
        "value_network = ValueNetwork(env.observation_space.shape[0], 16, 1)\n",
        "actor_network = ActorNetwork(env.observation_space.shape[0], 16, env.action_space.n)\n",
        "print(value_network)\n",
        "print(actor_network)\n",
        "\n",
        "# Define your optimizers\n",
        "value_network_optimizer = torch.optim.RMSprop(value_network.parameters(), lr=0.01)\n",
        "actor_network_optimizer = torch.optim.RMSprop(actor_network.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "num_iterations = 250   # How many update step do we perform   \n",
        "batch_size = 1024      # How many samples to collect\n",
        "gamma = 1\n",
        "lambda_entropy = 0.001 # regularization coefficient for entropy\n",
        "\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "    # Initialize batch storage\n",
        "    batch_losses = torch.zeros(batch_size)\n",
        "    batch_returns = np.zeros(batch_size)\n",
        "\n",
        "\n",
        "    states = np.empty((batch_size,) + env.observation_space.shape, dtype=np.float)        # shape (batch_size, state_dim)\n",
        "    rewards = np.empty((batch_size,), dtype=np.float)                                     # shape (batch_size, )                                 \n",
        "    next_states = np.empty((batch_size,) + env.observation_space.shape, dtype=np.float)   # shape (batch_size, state_dim)\n",
        "    dones = np.empty((batch_size,), dtype=np.bool)                                        # shape (batch_size, ) \n",
        "    proba = torch.empty((batch_size,), dtype=np.float)                                    # shape (batch_size, ), store pi(a_t|s_t)\n",
        "    next_value = 0                                \n",
        "  \n",
        "    # Intialize environment\n",
        "    state = env.reset()\n",
        "\n",
        "    # Generate batch\n",
        "    for i in range(batch_size):\n",
        "        action = ...\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        states[i] = ...\n",
        "        rewards[i] = ...\n",
        "        next_states[i] = ...\n",
        "        dones[i] = ...\n",
        "        proba[i] = ...\n",
        "\n",
        "        state = next_state\n",
        "        if done:\n",
        "          state = env.reset()\n",
        "\n",
        "    if not done:\n",
        "        next_value = value_network.value(next_states[-1]).detach().numpy()[0]\n",
        "\n",
        "    # compute returns (with bootstrapping)\n",
        "    returns = np.zeros((batch_size,), dtype=np.float)\n",
        "    ...\n",
        "\n",
        "    # compute advantage\n",
        "    values = value_network.value(states)\n",
        "    advantages = ... - values.detach().numpy().squeeze()\n",
        "\n",
        "    # Compute MSE\n",
        "    value_network_optimizer.zero_grad()\n",
        "    loss_value = ...\n",
        "    loss_value.backward()\n",
        "    value_network_optimizer.step()\n",
        "\n",
        "    # compute entropy term\n",
        "    ...\n",
        "\n",
        "    # Compute Actor Gradient\n",
        "    actor_network_optimizer.zero_grad()\n",
        "    loss_policy = ...\n",
        "    loss_policy.backward()\n",
        "    actor_network_optimizer.step()\n",
        "\n",
        "    # this code is to evaluate the policy every 10 iterations\n",
        "    if( (iteration+1)%10 == 0 ):\n",
        "        eval_rewards = np.zeros(5)\n",
        "        for sim in range(5):\n",
        "            eval_done = False\n",
        "            eval_state = eval_env.reset()\n",
        "            while not eval_done:\n",
        "                eval_action = actor_network.sample_action(eval_state)\n",
        "                eval_next_state, eval_reward, eval_done, _ = eval_env.step(eval_action)\n",
        "                eval_rewards[sim] += eval_reward\n",
        "                eval_state = eval_next_state\n",
        "        \n",
        "        print(\"it, rewards = \", \n",
        "              iteration +1, eval_rewards.mean())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0HW0jgTxdn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)\n",
        "for episode in range(4):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    while not done:\n",
        "        action = actor_network.sample_action(state)\n",
        "        state, reward, done, info = env.step(action)\n",
        "env.close()\n",
        "show_video(\"./gym-results\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}