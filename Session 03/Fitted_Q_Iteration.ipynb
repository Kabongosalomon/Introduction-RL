{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Fitted_Q_Iteration.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kabongosalomon/Introduction-RL/blob/master/Session%2003/Fitted_Q_Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1qj4EQb7anT",
        "colab_type": "text"
      },
      "source": [
        "### Google Colab setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyZ9J-Pl7anW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAcUywwj7and",
        "colab_type": "code",
        "outputId": "cd9ab013-0790-40ac-b744-97f8c9269a15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1\n",
        "!cd mvarl_hands_on && git pull"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcByt3G37ank",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './mvarl_hands_on/fitted_q_iteration')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gSry76D7anq",
        "colab_type": "text"
      },
      "source": [
        "# Fitted Q Iteration (FQI)\n",
        "\n",
        "We saw that, if $Q$ is the optimal value function, then it is the fixed point of the Bellman optimality operator\n",
        "\n",
        "$$\n",
        "Q(s, a) = T^*Q(s, a) = r(s, a) + \\gamma \\mathbb{E}_{s'\\sim p(\\cdot|s,a)}[\\max_{a'}Q(s', a')]\n",
        "$$\n",
        "and can be found by value iteration: if $Q_{k+1} = T^* Q_{k}$, then $Q_k \\to Q^*$.\n",
        "\n",
        "\n",
        "The goal of Fitted Q Iteration (FQI) is to search for an approximation of $Q$ in a function space $\\mathcal{F}$. It proceeds as value iteration but, instead of computing $T^* Q_{k}$ exactly, it looks for its best approximation in $\\mathcal{F}$\n",
        "\n",
        "$$\n",
        "Q_{k+1} = \\arg\\min_{f \\in \\mathcal{F}} \\mathbb{E}_{s, a \\sim \\mu}[ (f(s, a) - T^*Q_{k}(s,a))^2]\n",
        "$$\n",
        "where $\\mu$ is some probability distribution on the state-action space. In practice, we cannot compute expectations exactly, and we consider the following approximation based on sampling:\n",
        "\n",
        "$$\n",
        "Q_{k+1} = \\arg\\min_{f \\in \\mathcal{F}} \\sum_{i=1}^N [f(s_i, a_i) - (r_i + \\gamma\\max_{a'}Q_{k}(s'_i,a'))]^2\n",
        "$$\n",
        "for $s_i, a_i \\sim \\mu$ and $s'_i \\in p(\\cdot|s_i,a_i)$.\n",
        "\n",
        "The error between $Q_K$ and $Q^*$ can be bounded as a function of the number of iterations $K$, the number of samples used in each iteration $N$ and the properties of the function class $\\mathcal{F}$. \n",
        "\n",
        "\n",
        "# Linear FQI\n",
        "\n",
        "In this practical session, we consider Linear FQI, in which we have a feature function $\\phi: S \\times A \\to \\mathbb{R}^d$ and $\\mathcal{F} = \\{ f: f(s, a) = \\theta^T \\phi(s,a), \\theta \\in \\mathbb{R}^d\\}$.\n",
        "\n",
        "In this case, each $Q_k$ is represented by a vector $\\theta_k$ and the optimization problem to be solved in each iteration becomes\n",
        "\n",
        "$$\n",
        "\\theta_{k+1} = \\arg\\min_{\\theta \\in \\mathbb{R}^d} \\left( \\sum_{i=1}^N [\\theta^T \\phi(s_i, a_i) - (r_i + \\gamma\\max_{a'}\\theta_{k}^T\\phi(s'_i,a'))]^2 + \\lambda  \\theta^T \\theta \\right)\n",
        "$$\n",
        "\n",
        "where we included a regularization term.  Let $y_i^k = r_i + \\gamma\\max_{a'}\\theta_{k}^T\\phi(s'_i,a')$,  $y_k = (y_1^k, \\ldots, y_n^k)^T$, and $Z_{i, j} = \\phi_j(s_i, a_i)$. The solution to this optimization problem is\n",
        "\n",
        "$$\n",
        "\\theta_{k+1} = (Z^T Z + \\lambda I)^{-1} Z^T y_k\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1pho_lx7anr",
        "colab_type": "text"
      },
      "source": [
        "# Linear Quadratic Gaussian Regulator\n",
        "\n",
        "We consider an environment where the state space $[-10, 10]$, the action space is $[-8, 8]$, and such that the transitions and the rewards are given by\n",
        "\n",
        "$$\n",
        "s_{t+1} = A s_{t} + B a_{t} + n_t \\quad \\text{ and } \\quad r_t = -(Q s_{t}^2 + R a_t)\n",
        "$$\n",
        "\n",
        "respectively, where $n_t$ is a Gaussian noise. \n",
        "\n",
        "This is a 1d instantance of a Linear Quadratic Gaussian Regulator.\n",
        "\n",
        "We use the following feature function:\n",
        "\n",
        "$$\n",
        "\\phi(s, a) = (sa, s^2 + a^2, a)^T\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuskfEMg7ant",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from lqg1d import LQG1D\n",
        "from utils import plot_solution"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzGclFl67any",
        "colab_type": "text"
      },
      "source": [
        "# Step 1: Define parameters and collect data using a random policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9aGLt2X7anz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize LQG envinroment\n",
        "env = LQG1D()\n",
        "\n",
        "# Discretized states and actions\n",
        "states = discrete_states = np.linspace(-10, 10, 40)   # used for plotting only\n",
        "actions = discrete_actions = np.linspace(-8, 8, 20)   # used to compute argmax over actions\n",
        "\n",
        "# Total number of transitions to collect\n",
        "N_transitions = 100\n",
        "\n",
        "# Lenght of an episode\n",
        "horizon = 50\n",
        "\n",
        "# discount factor\n",
        "gamma = 0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwP7mKUi7an5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomPolicy:\n",
        "    \"\"\"\n",
        "    Defines a random policy. \n",
        "    \"\"\"\n",
        "    def __init__(self, actions):\n",
        "        self.actions = actions \n",
        "\n",
        "    def sample(self, state): \n",
        "        idx = np.random.randint(0, len(self.actions))\n",
        "        return self.actions[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d49QheYX7an-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_data(policy, env, n_data, horizon = np.inf):\n",
        "    \"\"\"\n",
        "    \n",
        "    TO BE COMPLETED\n",
        "    \n",
        "    \n",
        "    Collects data (state, action, next_state, reward) for a given environment\n",
        "    following a given policy\n",
        "\n",
        "    policy:  behavior policy\n",
        "    env:     environment\n",
        "    n_data:  number of points (state, action, next_state, reward) to collect\n",
        "    horizon: maximum length of an episode\n",
        "\n",
        "    returns: (states, actions, next_states, rewards, done_flags), where\n",
        "             - states and next_states are arrays of shape (n_data, state_dim)\n",
        "             - actions is an array of shape (n_data, action_dim)\n",
        "             - rewards is an array of shape (n_data,)\n",
        "             - done_flags is an array of shape (n_data,)\n",
        "    \"\"\"\n",
        "    # Getting the dimension of the state and the action spaces\n",
        "    state_dim = 1\n",
        "    action_dim = 1\n",
        "    if isinstance(env.observation_space, gym.spaces.Box):\n",
        "        state_dim = env.observation_space.shape[0]\n",
        "    if isinstance(env.action_space, gym.spaces.Box):\n",
        "        action_dim = env.observation_space.shape[0]\n",
        "\n",
        "    # Initializing arrays\n",
        "    states = np.zeros((n_data, state_dim))\n",
        "    next_states = np.zeros((n_data, state_dim))\n",
        "    actions = np.zeros((n_data, action_dim))\n",
        "    rewards = np.zeros(n_data)\n",
        "    done_flags = np.zeros(n_data)\n",
        "\n",
        "    # Gather data\n",
        "    state = env.reset()\n",
        "    step_count = 0\n",
        "    for tt in range(n_data):\n",
        "        action = policy.sample(state)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        states[tt, :]      = state\n",
        "        actions[tt, :]     = action\n",
        "        rewards[tt]        = reward\n",
        "        next_states[tt, :] = next_state\n",
        "        done_flags[tt]     = done\n",
        "\n",
        "        if done or step_count == horizon:\n",
        "          step_count = 0        # if we are done of reched the horizon\n",
        "          state = env.reset()   # we reset the env.\n",
        "          \n",
        "        state = next_state\n",
        "        step_count += 1\n",
        "\n",
        "\n",
        "    return states, actions, next_states, rewards, done_flags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFkGiewl7aoC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "81e209ac-3e04-491c-a0ae-a4017d601868"
      },
      "source": [
        "# collect data\n",
        "policy = RandomPolicy(actions)\n",
        "data = collect_data(policy, env, N_transitions, horizon)\n",
        "\n",
        "states, actions, next_states, rewards, done_flags = data\n",
        "# done_flags[ii] = 1 if states[ii] is terminal, and 0 otherwise\n",
        "print(actions[:5])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-7.15789474]\n",
            " [-2.10526316]\n",
            " [-2.10526316]\n",
            " [ 8.        ]\n",
            " [ 8.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBsN3OOv7aoH",
        "colab_type": "text"
      },
      "source": [
        "# Step 2: Define feature function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slGZ71QX7aoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LQG_FeatureFunction:\n",
        "    \"\"\"\n",
        "    TO BE COMPLETED\n",
        "    \n",
        "    Feature function for LQG\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        self.feature_dim = 3\n",
        "        self.state_dim = 1\n",
        "\n",
        "    def map(self, state, action):\n",
        "        feature = np.zeros(3)\n",
        "        feature[0] = state * action\n",
        "        feature[1] = state**2 + action**2\n",
        "        feature[2] = action \n",
        "        return feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RUch2Xb7aoN",
        "colab_type": "text"
      },
      "source": [
        "# Step 3: Run Linear FQI and compare its solution to the true Q function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzvSo7L27aoO",
        "colab_type": "code",
        "outputId": "eb917ce6-d911-42e9-cb4d-7af991e0a264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Feature map\n",
        "feature_map = LQG_FeatureFunction(env)\n",
        "\n",
        "# theta\n",
        "theta = np.zeros(feature_map.feature_dim)\n",
        "\n",
        "# FQI\n",
        "n_it = 10 \n",
        "Z_ij = np.zeros(N_transitions, N_transitions)\n",
        "\n",
        "for it in range(n_it):\n",
        "    # Compute matrix Z and targets y\n",
        "    for nn in range(N_transitions):   \n",
        "        Z_ij[it, nn] = feature_map.map(states[nn], actions[nn])\n",
        "        pass\n",
        "        \n",
        "    # Solve least squares problem to update theta\n",
        "\n",
        "# Plot solution\n",
        "plot_solution(env, feature_map, states, actions, theta, gamma)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.LQG_FeatureFunction at 0x7efcdda56b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymBxpXZYHu2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95f86666-a9a8-4aff-9478-3a3db65b96ab"
      },
      "source": [
        "len(states)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhKL175l7aoS",
        "colab_type": "text"
      },
      "source": [
        "# Step 4: Execute the policy\n",
        "\n",
        "Write a loop to run the learned policy for $m$ episodes and compute the total sum of rewards obtained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fse36AZn7aoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}