{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kabongosalomon/Introduction-RL/blob/master/Session%2003/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gow745pr7ZQY",
        "colab_type": "text"
      },
      "source": [
        "## Colab setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZdOcPcY7ZQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwbgt65d7ZQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRWw4bZ87ZQo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3c6ba45-6371-4f32-be9c-e4a41f2e68d1"
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (45.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd3geSA67ZQu",
        "colab_type": "text"
      },
      "source": [
        "# Deep Q-Learning (DQN)\n",
        "\n",
        "\n",
        "In DQN, the $Q$-function is parameterized by a neural network of parameters $\\theta$. The network takes as input a state $s$ and outputs $Q(s, a, \\theta)$ for all actions $a$. \n",
        "\n",
        "The network is trained in way that is similar to Fitted Q Iteration. At each time $T$, the agent has observed the transitions $(s_t, a_t, r_t, s_t')_{t=1}^T$, which are stored in a __replay buffer__.\n",
        "\n",
        "In addition to the network with parameters $\\theta$, DQN keeps another network with the same architecture and parameters $\\tilde{\\theta}$, called __target network__. \n",
        "To update the parameters $\\theta$, we sample $N$ transitions from the __replay buffer__, we define the loss \n",
        "\n",
        "$$\n",
        "L(\\theta) = \\sum_{i=1}^N [Q(s_i, a_i, \\theta) - (r_i + \\gamma\\max_{a'}Q(s'_i,a', \\tilde{\\theta}))]^2\n",
        "$$\n",
        "\n",
        "and update \n",
        "\n",
        "$$\n",
        "\\theta \\gets \\theta + \\eta \\nabla L(\\theta).\n",
        "$$\n",
        "\n",
        "\n",
        "Every $C$ iterations, the target network is updated as $\\tilde{\\theta} \\gets \\theta$. \n",
        "\n",
        "At each time $t$, DQN updates the networks as described above, selects an action according to an $\\epsilon$-greedy policy, plays the action and stores the new data in the replay buffer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpj_PvaS7ZQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from copy import deepcopy\n",
        "\n",
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import clear_output\n",
        "from pathlib import Path\n",
        "\n",
        "import random, os.path, math, glob, csv, base64, itertools, sys\n",
        "from pprint import pprint\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import io\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8kIhFPo7ZQ1",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Define the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjo8iA9j7ZQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Environment\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# Discount factor\n",
        "GAMMA = 0.99\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 256\n",
        "# Capacity of the replay buffer\n",
        "BUFFER_CAPACITY = 10000\n",
        "# Update target net every ... episodes\n",
        "UPDATE_TARGET_EVERY = 20\n",
        "\n",
        "# Initial value of epsilon\n",
        "EPSILON_START = 1.0\n",
        "# Parameter to decrease epsilon\n",
        "DECREASE_EPSILON = 200\n",
        "# Minimum value of epislon\n",
        "EPSILON_MIN = 0.05\n",
        "\n",
        "# Number of training episodes\n",
        "N_EPISODES = 200\n",
        "\n",
        "# Learning rate\n",
        "LEARNING_RATE = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Lg3iIs7ZQ9",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Define the replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMdSUwIb7ZQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, state, action, reward, next_state):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = (state, action, reward, next_state)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bxHR5u57ZRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create instance of replay buffer\n",
        "replay_buffer = ReplayBuffer(BUFFER_CAPACITY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJDq4p7V7ZRH",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Define the neural network architecture, objective and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2M8VINN7ZRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic neural net.\n",
        "    \"\"\"\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPRSqaF97ZRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create network and target network\n",
        "hidden_size = 128\n",
        "obs_size = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "q_net = Net(obs_size, hidden_size, n_actions)\n",
        "target_net = Net(obs_size, hidden_size, n_actions)\n",
        "\n",
        "# objective and optimizer\n",
        "objective = nn.MSELoss()\n",
        "optimizer = optim.Adam(params=q_net.parameters(), lr=LEARNING_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKRalZYH7ZRT",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Implement DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5FsQLyW7ZRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "#  Some useful functions\n",
        "#\n",
        "\n",
        "def get_q(states):\n",
        "    \"\"\"\n",
        "    Compute Q function for a list of states\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        states_v = torch.FloatTensor([states])\n",
        "        output = q_net.forward(states_v).data.numpy()  # shape (1, len(states), dim_state)\n",
        "    return output[0, :, :]  # shape (len(states), dim_state)\n",
        "\n",
        "def dqn_policy(s):\n",
        "    \"\"\"\n",
        "    Choose action at s using q_net\n",
        "    \"\"\"\n",
        "    if not isinstance(s, np.ndarray):\n",
        "        raise TypeError(\"Vector encoding expected for state variable.\")\n",
        "    q = get_q([s])[0]\n",
        "    return q.argmax()\n",
        "\n",
        "def eval_dqn(n_sim=5):\n",
        "    \"\"\"\n",
        "    Monte Carlo evaluation of DQN agent\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    copy_env = deepcopy(env)\n",
        "    for sim in range(n_sim):\n",
        "        done = False\n",
        "        state = copy_env.reset()\n",
        "        sim_rewards = 0.0\n",
        "        while not done:\n",
        "            action = dqn_policy(state)\n",
        "            next_state, reward, done, _ = copy_env.step(action)\n",
        "            sim_rewards += reward\n",
        "            state = next_state\n",
        "        rewards.append(sim_rewards)\n",
        "    return rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06fbcv507ZRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def choose_action(state, epsilon):\n",
        "    \"\"\"\n",
        "    TO BE IMPLEMENTED\n",
        "    \n",
        "    Return action according to an epsilon-greedy exploration policy\n",
        "    \"\"\"\n",
        "    q_state = get_q([state])[0] # array of shape (n_actions,)\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "      action = env.action_space.sample() # random action\n",
        "    else:\n",
        "      action = q_state.argmax()\n",
        "    return action\n",
        "    \n",
        "\n",
        "def update(state, action, reward, next_state, done):\n",
        "    \"\"\"\n",
        "    TO BE COMPLETED\n",
        "    \"\"\"\n",
        "    \n",
        "    # add data to replay buffer\n",
        "    if done:\n",
        "        next_state = None\n",
        "    replay_buffer.push(state, action, reward, next_state)\n",
        "    \n",
        "    if len(replay_buffer) < BATCH_SIZE:\n",
        "        return np.inf\n",
        "    \n",
        "    # get batch\n",
        "    # transitions = list of (state, action, reward, next_state)\n",
        "    transitions = replay_buffer.sample(BATCH_SIZE)\n",
        "\n",
        "    # 1st thing: compute Q(s_i, a_i, theta) for all (s_i, a_i)\n",
        "    # in the batch\n",
        "    \n",
        "    # Build tensor with s_i and tensor with a_i\n",
        "    batch_states = torch.FloatTensor( \n",
        "                    [ transitions[ii][0] for ii in range(BATCH_SIZE) ]\n",
        "                    )\n",
        "    batch_actions = torch.LongTensor(  # type is important (Long) \n",
        "                    [ transitions[ii][1] for ii in range(BATCH_SIZE) ]\n",
        "                    )\n",
        "    batch_rewards = torch.FloatTensor( \n",
        "                    [ transitions[ii][2] for ii in range(BATCH_SIZE) ]\n",
        "                    )\n",
        "\n",
        "    non_final_mask = torch.tensor([(transitions[ii][3] is not None) \n",
        "                                   for ii in range(BATCH_SIZE)], dtype=torch.bool)\n",
        "    non_final_next_states = torch.FloatTensor(\n",
        "            [transitions[ii][3]  for ii in range(BATCH_SIZE) if transitions[ii][3] is not None])\n",
        "        \n",
        "    next_state_values = torch.zeros(BATCH_SIZE)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "    state_action_values = q_net(batch_states).gather(1, batch_actions.view(-1, 1))\n",
        "\n",
        "\n",
        "    # # A simpler (and slower) way to compute all these things\n",
        "    # for ii in range(BATCH_SIZE):\n",
        "    #   state_ii = transitions[ii][0]\n",
        "    #   action_ii = transitions[ii][1]\n",
        "    #   reward_ii = transitions[ii][2]\n",
        "    #   next_state_ii = transitions[ii][3]\n",
        "      \n",
        "    #   next_value = 0\n",
        "    #   if next_state_ii is not None:\n",
        "    #     next_value = # max_a Q(next_state, a) with target net\n",
        "      \n",
        "\n",
        "    #   values[ii] = # Q(state_ii)[action_ii]  with q_net\n",
        "    #   targets[ii] = reward_ii + GAMMA*next_value\n",
        "\n",
        "    \n",
        "\n",
        "    # Compute loss - TO BE IMPLEMENTED!\n",
        "    values  = state_action_values\n",
        "    targets = batch_rewards + GAMMA*next_state_values\n",
        "    loss = objective(values, targets)\n",
        "     \n",
        "    # Optimize the model - UNCOMMENT!\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    return loss.data.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn9SCgUE7ZRd",
        "colab_type": "code",
        "outputId": "a31601d0-18d0-44f2-c5d5-a7de04990e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "#\n",
        "# Train\n",
        "# \n",
        "\n",
        "EVAL_EVERY = 5\n",
        "REWARD_THRESHOLD = 199\n",
        "\n",
        "def train():\n",
        "    state = env.reset()\n",
        "    epsilon = EPSILON_START\n",
        "    ep = 0\n",
        "    total_time = 0\n",
        "    while ep < N_EPISODES:\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # take action and update replay buffer and networks\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        loss = update(state, action, reward, next_state, done)\n",
        "\n",
        "        # update state\n",
        "        state = next_state\n",
        "\n",
        "        # end episode if done\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            ep   += 1\n",
        "            if ( (ep+1)% EVAL_EVERY == 0):\n",
        "                rewards = eval_dqn()\n",
        "                print(\"episode =\", ep+1, \", reward = \", np.mean(rewards),\n",
        "                      \"loss = \", loss)\n",
        "                if np.mean(rewards) >= REWARD_THRESHOLD:\n",
        "                    break\n",
        "\n",
        "            # update target network\n",
        "            if ep % UPDATE_TARGET_EVERY == 0:\n",
        "                target_net.load_state_dict(q_net.state_dict())\n",
        "            # decrease epsilon\n",
        "            epsilon = EPSILON_MIN + (EPSILON_START - EPSILON_MIN) * \\\n",
        "                            np.exp(-1. * ep / DECREASE_EPSILON )    \n",
        "\n",
        "        total_time += 1\n",
        "\n",
        "train()\n",
        "rewards = eval_dqn(20)\n",
        "print(\"\")\n",
        "print(\"mean reward after training = \", np.mean(rewards))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode = 5 , reward =  9.2 loss =  3.3576632\n",
            "episode = 10 , reward =  10.2 loss =  2.159085\n",
            "episode = 15 , reward =  9.6 loss =  1.9743259\n",
            "episode = 20 , reward =  8.8 loss =  3.554939\n",
            "episode = 25 , reward =  9.6 loss =  4.0824556\n",
            "episode = 30 , reward =  9.4 loss =  2.0518126\n",
            "episode = 35 , reward =  10.0 loss =  5.0230875\n",
            "episode = 40 , reward =  9.8 loss =  5.008074\n",
            "episode = 45 , reward =  9.4 loss =  2.6171155\n",
            "episode = 50 , reward =  9.2 loss =  3.350712\n",
            "episode = 55 , reward =  9.6 loss =  3.5971994\n",
            "episode = 60 , reward =  9.0 loss =  4.1260204\n",
            "episode = 65 , reward =  9.4 loss =  5.1970677\n",
            "episode = 70 , reward =  9.6 loss =  3.4817276\n",
            "episode = 75 , reward =  9.6 loss =  4.8846965\n",
            "episode = 80 , reward =  10.0 loss =  3.748524\n",
            "episode = 85 , reward =  9.2 loss =  5.368074\n",
            "episode = 90 , reward =  9.2 loss =  4.733005\n",
            "episode = 95 , reward =  9.8 loss =  4.418245\n",
            "episode = 100 , reward =  9.4 loss =  5.086237\n",
            "episode = 105 , reward =  9.0 loss =  7.7373652\n",
            "episode = 110 , reward =  9.2 loss =  3.3070984\n",
            "episode = 115 , reward =  9.2 loss =  3.9999769\n",
            "episode = 120 , reward =  9.2 loss =  4.713021\n",
            "episode = 125 , reward =  9.8 loss =  6.2543683\n",
            "episode = 130 , reward =  9.2 loss =  3.6425562\n",
            "episode = 135 , reward =  9.0 loss =  3.6761775\n",
            "episode = 140 , reward =  8.8 loss =  7.357523\n",
            "episode = 145 , reward =  9.8 loss =  5.3463893\n",
            "episode = 150 , reward =  9.4 loss =  5.345378\n",
            "episode = 155 , reward =  9.4 loss =  6.9550767\n",
            "episode = 160 , reward =  9.0 loss =  4.1720486\n",
            "episode = 165 , reward =  9.2 loss =  6.88448\n",
            "episode = 170 , reward =  9.8 loss =  6.8797398\n",
            "episode = 175 , reward =  10.0 loss =  2.8827798\n",
            "episode = 180 , reward =  9.4 loss =  8.138717\n",
            "episode = 185 , reward =  9.2 loss =  6.049507\n",
            "episode = 190 , reward =  9.6 loss =  7.7576265\n",
            "episode = 195 , reward =  9.6 loss =  4.748582\n",
            "episode = 200 , reward =  9.6 loss =  5.1407037\n",
            "\n",
            "mean reward after training =  9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xshtpttN7ZRi",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3UslTe87ZRj",
        "colab_type": "code",
        "outputId": "d0d5aea6-064b-48c5-d643-4a46deb83922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "def show_video(directory):\n",
        "    html = []\n",
        "    for mp4 in Path(directory).glob(\"*.mp4\"):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append('''<video alt=\"{}\" autoplay \n",
        "                      loop controls style=\"height: 400px;\">\n",
        "                      <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                 </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "    \n",
        "def make_seed(seed):\n",
        "    np.random.seed(seed=seed)\n",
        "    torch.manual_seed(seed=seed)\n",
        "  \n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1005'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1005'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLWwf5FM7ZRo",
        "colab_type": "code",
        "outputId": "aba1c59b-7191-4f9b-cc5b-dd4e39fb0367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "env = Monitor(env, \"./gym-results\", force=True, video_callable=lambda episode: True)\n",
        "for episode in range(1):\n",
        "    done = False\n",
        "    state = env.reset()\n",
        "    while not done:\n",
        "        action = dqn_policy(state)\n",
        "        state, reward, done, info = env.step(action)\n",
        "env.close()\n",
        "show_video(\"./gym-results\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"gym-results/openaigym.video.1.875.video000000.mp4\" autoplay \n",
              "                      loop controls style=\"height: 400px;\">\n",
              "                      <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAACWhtZGF0AAACrgYF//+q3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1MiByMjg1NCBlOWE1OTAzIC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAxNyAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTMgbG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTI1IHNjZW5lY3V0PTQwIGludHJhX3JlZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAuNjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAABkmWIhAAv//72rvzLK0cLlS4dWXuzUfLoSXL9iDB9aAAAAwAAAwAAJuKiZ0WFMeJsgAAALmAIWElDyDzETFWKgSxIZq+DFQoBVnP+2BY5KHnsdIRFcc3mroscfOLJaWfE65+/j+aIf+mZb0lqyy6Y1XTojbLoOvMaYvG7D4/UcxtAVPmKYFnGR7303IruYANSrx241HEzoNoiM17JXSbd788edBQq3yb1MGRF94LF6LyOcSWcfOoiy5QAsK3UXZY8P5AOgkA7d+bAzeBnvFGlyJjZMY+P1PWWb4wBrNxQg+jtmHxixt+KzeYQKXXCVnrkzklwVdGIf2WrW0FkL4Lxh1dmtu37NwdTwABsEIQSXxizQ7RAzCtwB9q779yn5eFyiW/LhGwfNGjmnNhrflmEdt5uj9VlWtXLFDNRwosPBevo+xsJrIqMrS4Groabk1Pq4FxyqEaU8dChYB7WxnNiAGFyfJKEkiHRBjliNUSGtaBW1Irjpp8fK9lwYkwNNWM272qwEkNLNloD+AAAAwAAAwACAwAAAKRBmiRsQr/+OEAAAQ3EqCIpYAOHiDrN8xU/gK5yRDJLyZFBeg9f3Y7lT6Fpn1kNl0nY/PrkR/GsuWV8UZLfmRosb03mBJt3UNpt8zR7BpmtjDpzmpEU2vMs2YmVFCurPQlfbuPekqx67QgCw5v2tzjywnX667m259v5iN6y/PoFXi11INXqgMrjplI9twjqCp3X/Qv+jXuP/wwCbvtU0SvROADHgAAAAG1BnkJ4hH8AABZsHO8x4AiUy07/I9oHfZc30FZXr24hDayGheSts6DMf+euhvlcx9IDFxm8C9AkEpwSdK4xq0VxB8oiX6cVqG1F0ZqFolviMMsFR8l7mikJbcORIh9CnbuwAAAq613u1oywANmBAAAAWgGeYXRH/wAADdXdFAkUrAzXYOQJ0zIAC2uMO0a6QLOzfkU3vWj1CXdb4AY+PGT6OBw1z95Vldx4BE053nvC0Ot0XKHENUo/2iw5wfnQAAADACuQQvtTwgANmAAAAGQBnmNqR/8AACO/H4/45RWWqpNEir32FZ0w0i4lX3MA4hyVr4vLiRQAfztIWzJ/XH7ccp0F0US0VZTcZ9fMdaIboRXyWj2UqNQWVY4u4urAlAgkW5ByrRoZgABEJ4A+2p4QAH+BAAAA0EGaZkmoQWiZTBTwn/3xAAADAp75vxgAOKVBj6cIRkZv5QMneUum/viHJRM1MhlXP44FqmYxhME1pmTuaZq7/7inGovteV4AVOlzf+TGPvYX2DaKtP/tp7nscD8GVyHxU42//pIk4No7wazyEvb0RvNLZ1K6IFHWQcMVK8o8odn4MaAxRnWq9ahqDAuh5TgoHGYfx0HLlJQqYYGl3/mbBIWuluiC4DbZgf8/MYfbR+u1DIQIVCbPKY5SX2aJ+XD7iTLmIV3F3HoyhdalUESAUEEAAABlAZ6Fakf/AAAjkieaOvNX3YJTfIRc0KMQiAU2UwqzKroG95wbumQc7XTT5DpFwZI6RxrPw0IgA2YIA7H1gyE/pkWp4+i/kazwAAADABBHviFuUDZb2tS4j4FJgV37vO921PCAAz8AAACZQZqHSeEKUmUwIT/98QAAAwKzVlDa8Acph4CHaVdjF9CZlKIP15BbHQIQrv8Tm+Db+fl+jkLaC6U3tVcIiIAhCW1jIH0Pzc/n6gGQkhkCkj93g7ibZPasXGIBHYqowZepnvbLSGd4slqIdWEC0L13TPFtHPAj+G7a2fzCRsqxGTOe/eyZkIDYmzx0jsGRH/4YfBQ1LMgj4AW1AAAAwUGaqEnhDomUwIR//eEAAAQ0hxvahCQAjhEhe9m7xtZP7vZPrRhtCqx7dqpoW7lsMMYBjGzaI0ItJq6MjSiwkp8ZjrDzI8xdX/uQ6qE4l9367xLJvgjtioeTZKC7TvkeZcW5p0iGh//uwBtTb6Irm5b6/F1Gm+jTXPW8oin6oZI7qWqA1ud72vUMEhrgRvB3i2YeT1nUimujn4jTjQ8UJ3MdP6U+bYxMru7dePVhfWPpF+7kps6g8B1Gm/NTMTGERcAAAACWQZrJSeEPJlMCP//8hAAAEG9apaPLsAIPBoJL2sj8+P2pnxYtaNPZgaC3IFn5tTfEBL9aosBXhGNODNkJuJ/NX1FrBPK5RzW9S9k0ELLGzjUNeDtyXy1ZhabIKX2ZN1yFiiQ0Q09u+NksMMNY+g/b72ETyRoIqRSurUClOHjgLI/P+vLxm5NKFmnjoA1j1VTOQlAJ6BLwAAADe21vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAAADIAAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAAKldHJhawAAAFx0a2hkAAAAAwAAAAAAAAAAAAAAAQAAAAAAAADIAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAJYAAABkAAAAAAAJGVkdHMAAAAcZWxzdAAAAAAAAAABAAAAyAAAAgAAAQAAAAACHW1kaWEAAAAgbWRoZAAAAAAAAAAAAAAAAAAAMgAAAAoAVcQAAAAAAC1oZGxyAAAAAAAAAAB2aWRlAAAAAAAAAAAAAAAAVmlkZW9IYW5kbGVyAAAAAchtaW5mAAAAFHZtaGQAAAABAAAAAAAAAAAAAAAkZGluZgAAABxkcmVmAAAAAAAAAAEAAAAMdXJsIAAAAAEAAAGIc3RibAAAAJhzdHNkAAAAAAAAAAEAAACIYXZjMQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAJYAZAASAAAAEgAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABj//wAAADJhdmNDAWQAH//hABlnZAAfrNlAmDPl4QAAAwABAAADAGQPGDGWAQAGaOvjyyLAAAAAGHN0dHMAAAAAAAAAAQAAAAoAAAEAAAAAFHN0c3MAAAAAAAAAAQAAAAEAAABQY3R0cwAAAAAAAAAIAAAAAQAAAgAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAMAAAAAAQAAAQAAAAADAAACAAAAABxzdHNjAAAAAAAAAAEAAAABAAAACgAAAAEAAAA8c3RzegAAAAAAAAAAAAAACgAABEgAAACoAAAAcQAAAF4AAABoAAAA1AAAAGkAAACdAAAAxQAAAJoAAAAUc3RjbwAAAAAAAAABAAAAMAAAAGJ1ZHRhAAAAWm1ldGEAAAAAAAAAIWhkbHIAAAAAAAAAAG1kaXJhcHBsAAAAAAAAAAAAAAAALWlsc3QAAAAlqXRvbwAAAB1kYXRhAAAAAQAAAABMYXZmNTcuODMuMTAw\" type=\"video/mp4\" />\n",
              "                 </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}