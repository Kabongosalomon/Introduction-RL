{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4-final"
    },
    "colab": {
      "name": "AIMS_2020 Exercice.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kabongosalomon/Introduction-RL/blob/master/Session%2001/AIMS_2020_Exercice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QYW2YAMZOX4-"
      },
      "source": [
        "# Reinforcement Learning in Finite Markov Decision Processes MDP)ss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "496PYAVFF-8Z",
        "colab_type": "code",
        "outputId": "ebb14e3d-2c07-4dcd-d0fa-d3c85f7bc862",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/Kabongosalomon/Introduction-RL.git > /dev/null 2>&1\n",
        "!cd Introduction-RL && git pull"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NNsuKZxeOX4_"
      },
      "source": [
        "## MDPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uQU88u1F-8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './Introduction-RL/tools/utils')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import math\n",
        "from gridworld import GridWorldWithPits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "puk7xD1EBDEu"
      },
      "source": [
        "## Define the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D61EyrSF-8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from finite_env import FiniteEnv\n",
        "import numpy as np\n",
        "\n",
        "class RobotEnv(FiniteEnv):\n",
        "    \"\"\"\n",
        "    Enviroment with 2 states and 3 actions\n",
        "    Args:\n",
        "        gamma (float): discount factor\n",
        "        seed    (int): Random number generator seed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gamma=0.5, seed=42):\n",
        "        # Set seed\n",
        "        self.RS = np.random.RandomState(seed)\n",
        "\n",
        "        # Transition probabilities\n",
        "        # shape (Ns, Na, Ns)\n",
        "        # P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a)\n",
        "\n",
        "        Ns = 2\n",
        "        Na = 3\n",
        "        \n",
        "        # P = np.array([[[1, 0], [3/4, 1/4], [1, 0]], [[0,1],[1,0], [1,0]]])      # |SxAxS|\n",
        "        P = np.array([[[1, 0], [3/4, 1/4], [0, 0]], [[0,1],[1,0], [1,0]]]) \n",
        "\n",
        "        self._R = np.array([[0,1,-0.5], [0, -1, 0]])                            # |SxA|\n",
        "\n",
        "        self.state_decoder  = {0: \"A\", 1: \"B\"}\n",
        "        self.action_decoder = {0: \"WAIT\", 1: \"SEARCH\", 2: \"RECHARGE\"}\n",
        "        \n",
        "        # Initialize base class\n",
        "        states = np.arange(Ns).tolist()\n",
        "        action_sets = [np.arange(Na).tolist()]*Ns\n",
        "        super().__init__(states, action_sets, P, gamma)\n",
        "\n",
        "    def reward_func(self, state, action, *_):\n",
        "        return self._R[state, action]\n",
        "\n",
        "    def reset(self, s=0):\n",
        "        self.state = s\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state = self.sample_transition(self.state, action)\n",
        "        reward = self.reward_func(self.state, action, next_state)\n",
        "        done = False\n",
        "        info = {\"str\" : \"In {} do {} arrive at {} get {}\".format(\n",
        "            self.state_decoder[state],\n",
        "            self.action_decoder[action],\n",
        "            self.state_decoder[next_state],\n",
        "            reward )}\n",
        "        self.state = next_state\n",
        "\n",
        "        observation = next_state\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def sample_transition(self, s, a):\n",
        "        prob = self.P[s,a,:]\n",
        "        s_ = self.RS.choice(self.states, p = prob)\n",
        "        return s_\n",
        "\n",
        "    def render_policy(self, policy):\n",
        "      if len(np.array(policy).shape) > 1:\n",
        "        policy = densify_policy(policy)\n",
        "\n",
        "      txt = \"\"\n",
        "      for i, a in enumerate(policy):\n",
        "        txt += \"In state {} perform {}\\n\".format(self.state_decoder[i], self.action_decoder[a])\n",
        "      return txt[:-1]\n",
        "\n",
        "    @property\n",
        "    def R(self):\n",
        "        return self._R\n",
        "  \n",
        "env = RobotEnv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4HE4FxvF-8v",
        "colab_type": "code",
        "outputId": "27587f21-028f-476b-d5ad-235c2bd862f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "env.P.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hdzoq4dLOX5C"
      },
      "source": [
        "Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-7bWMB0F-82",
        "colab_type": "code",
        "outputId": "6140cd11-6be7-418f-9722-294918e60430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "env.P"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1.  , 0.  ],\n",
              "        [0.75, 0.25],\n",
              "        [0.  , 0.  ]],\n",
              "\n",
              "       [[0.  , 1.  ],\n",
              "        [1.  , 0.  ],\n",
              "        [1.  , 0.  ]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYf_KqxNF-89",
        "colab_type": "code",
        "outputId": "b679454a-ddaa-46e9-d097-083541585a90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        }
      },
      "source": [
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.P.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.R.shape)  \n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Usefult methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=0, a=1): \", env.reward_func(0,1))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "print(env.render_policy(policy))\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = policy[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward, \"   --> \" + info[\"str\"] if \"str\" in info else \"\") \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Set of states: [0, 1]\n",
            "Set of actions: [0, 1, 2]\n",
            "Number of states:  2\n",
            "Number of actions:  3\n",
            "P has shape:  (2, 3, 2)\n",
            "R has shape:  (2, 3)\n",
            "discount factor:  0.5\n",
            "\n",
            "initial state:  0\n",
            "reward at (s=0, a=1):  1.0\n",
            "\n",
            "random policy =  [2 2]\n",
            "In state A perform RECHARGE\n",
            "In state B perform RECHARGE\n",
            "(s, a, s', r):\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-bab5267eb813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"   --> \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"str\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"str\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-07f6f44baedc>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-07f6f44baedc>\u001b[0m in \u001b[0;36msample_transition\u001b[0;34m(self, s, a)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0ms_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: probabilities do not sum to 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7YM7lLbRA68A"
      },
      "source": [
        "## Useful functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOZQiQmlF-9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparsify_policy(policy, Na):\n",
        "  ### Turn a dense policy into a sparse one.\n",
        "  #  Ex: [0, 1], Na=3  -> [[1, 0, 0], [0, 1, 0]]\n",
        "  ###\n",
        "\n",
        "  Ns = len(policy)\n",
        "  sparse_policy = np.zeros(shape=(Ns, Na))\n",
        "  for i, a in enumerate(policy):\n",
        "    sparse_policy[i,a]=1\n",
        "  return sparse_policy\n",
        "\n",
        "def densify_policy(policy):\n",
        "  ### Turn a sparse determinist policy into a dense one.\n",
        "  #  Ex: [[1, 0, 0], [0, 1, 0]] -> [0, 1]\n",
        "  ###\n",
        "  return np.array(policy).argmax(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4dyTP0kkAXKo"
      },
      "source": [
        "## Exercice : Policy Evaluation\n",
        "1. Evaluate the policy by solving the linear system\n",
        "2. Evaluate the policy through recursion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igl7Vj-5F-9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Policy evaluation (exact)\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.P\n",
        "R = env.R\n",
        "gamma = env.gamma\n",
        "\n",
        "\n",
        "# Policy to evaluate\n",
        "# State A: Search\n",
        "# State B: Wait\n",
        "dense_policy = np.array([1, 0])\n",
        "pi = sparsify_policy(dense_policy, Na=env.Na)\n",
        "print(\"## old pi:\")\n",
        "print(pi)\n",
        "print(env.render_policy(pi))\n",
        "\n",
        "\n",
        "# Compute the dynamics given the policy\n",
        "\n",
        "# using for loops\n",
        "# actions = [0, 1, 2]\n",
        "# states = [0, 1]\n",
        "# Ppi = np.zeros([2, 2])\n",
        "# for s in states:\n",
        "#     for s_next in stages :\n",
        "#         Ppi[s, s_next] = np.sum(pi[s]* P[s, :, s_next])\n",
        "\n",
        "\n",
        "# Using Matrix \n",
        "\n",
        "\n",
        "Ppi = np.sum(P * np.expand_dims(pi, axis=-1), axis =1)  # sum_a pi(a/s) P(s, a, s')\n",
        "                             \n",
        "Rpi = np.sum(R* pi, axis = 1)                           # sum_a^A pi(a/s) r(s, a)\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the policy\n",
        "Vpi = np.linalg.inv(np.identity(env.Ns)- gamma*Ppi).dot(Rpi)\n",
        "\n",
        "print(\"## Vpi: \", Vpi)\n",
        "\n",
        "\n",
        "# Compute the Q values\n",
        "Qpi = R + gamma * P.dot(Vpi)\n",
        "\n",
        "print(\"## Qpi:\")\n",
        "print(Qpi)\n",
        "\n",
        "\n",
        "# What is the next policy if we perform one step of policy improvment ?\n",
        "pi_new = Qpi.argmax(axis=1)\n",
        "\n",
        "print(\"## new pi:\")\n",
        "print(sparsify_policy(pi_new, Na=env.Na))\n",
        "print(env.render_policy(pi_new))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNZuM-mlF-9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Shape pi (expended) : \\n{np.expand_dims(pi, axis=-1).shape}\\n')\n",
        "print(f'Shape P : \\n{P.shape}\\n')\n",
        "print(f'Shape P*pi : \\n{(P*np.expand_dims(pi, axis=-1)).shape}\\n')\n",
        "print('-------------------------------------------------------------------\\n')\n",
        "print(f'Shape R : \\n{R.shape}\\n')\n",
        "print(f'Shape pi (expended) : \\n{pi.shape}\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnZfkM_gF-9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(P)\n",
        "print()\n",
        "print(np.expand_dims(pi, axis=-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ_9RtWlF-9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(P*np.expand_dims(pi, axis=-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kR5UrbpF-9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.sum(P*np.expand_dims(pi, axis=-1), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACf_6nMPF-9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Policy evaluation (recursive)\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.P\n",
        "R = env.R\n",
        "gamma = env.gamma\n",
        "\n",
        "\n",
        "# Policy to evaluate\n",
        "dpi = np.array([1, 0])\n",
        "pi = sparsify_policy(dpi, Na=env.Na)\n",
        "\n",
        "print(pi)\n",
        "\n",
        "# Stopping criterion -> maximum number of steps\n",
        "epsilon = 1e-3\n",
        "\n",
        "rmax = 1                    # max(|R|) R is a matrix\n",
        "\n",
        "max_k = int(np.log(rmax/epsilon)/np.log(1/gamma))+1\n",
        "\n",
        "v = np.zeros(env.Ns)\n",
        "\n",
        "for k in range(max_k):\n",
        "    v = Rpi + gamma* Ppi.dot(v)\n",
        "    print(k, v)\n",
        "\n",
        "print(f'final v : {v}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHpn3AOjF-9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stopping criterion -> compute the infinite norm\n",
        "epsilon = 1e-3 \n",
        "\n",
        "v_new = np.array([0, 0])\n",
        "v_old = np.array([2*epsilon, 0])\n",
        "first = True\n",
        "while np.absolute(v_new - v_old).max() > epsilon :\n",
        "    # if not first :\n",
        "    v_new, v_old = Rpi + gamma* Ppi.dot(v_new), v_new\n",
        "    print(v_new)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u0nl8OfLwpdC"
      },
      "source": [
        "## New Environment!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxS3iiOyF-9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# New Maze environment\n",
        "# s: start\n",
        "# g: goal\n",
        "# x: negative reward state\n",
        "\n",
        "grid1 = [\n",
        "    ['', '', '', 'g'],\n",
        "    ['', 'x', '', ''],\n",
        "    ['s', '', '', '']\n",
        "]\n",
        "grid1_MAP = [\n",
        "    \"+-------+\",\n",
        "    \"| : : :G|\",\n",
        "    \"| :x: : |\",\n",
        "    \"|S: : : |\",\n",
        "    \"+-------+\",\n",
        "]\n",
        "\n",
        "env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8dOl5rkF-9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.P.shape)  # P[s'|s,a] = P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"R has shape: \", env.R.shape)  \n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Usefult methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=0, a=1,s'=1): \", env.reward_func(0,1,1))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "dpi = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", dpi)\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = dpi[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward) \n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCpQyPRsF-91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dpi = np.random.randint(env.Na, size = (env.Ns,))\n",
        "env.render_policy(dpi)\n",
        "\n",
        "state = env.reset()\n",
        "env.render()\n",
        "for i in range(5):\n",
        "    action = dpi[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2ugaKsHsOX5G"
      },
      "source": [
        "## Exercice: Value Iteration\n",
        "1. Write a function applying the optimal Bellman operator on a provided Q function: $Q_1 = LQ_0, \\; Q_0\\in \\mathbb{R}^{S\\times A}$\n",
        "2. Write a function implementing Value Iteration (VI) with $\\infty$-norm stopping condition \n",
        "3. Evaluate the convergence of your estimate, i.e., plot the value $\\|V_n - V^\\star\\|_{\\infty}$\n",
        "4. Evaluate the convergence of your estimate, i.e., plot the value $\\|\\pi_n - \\pi^\\star\\|_{\\infty}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlzYTnJfF-96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# useful function\n",
        "def plot_infnorm(lst, star, name=\"V\"):\n",
        "  \n",
        "  lst = np.array(lst)\n",
        "  star = np.array(star)\n",
        "\n",
        "  # Compute inf norm\n",
        "  diff = np.absolute(lst - star).max(axis=1)\n",
        "  plt.figure()\n",
        "  plt.plot(diff)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Error')\n",
        "  plt.title(\"||{} - {}*||_inf\".format(name, name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3GleiYNF--B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute Value Iteration\n",
        "\n",
        "# Retrieve the environment MDP\n",
        "P = env.P\n",
        "R = env.R\n",
        "gamma = env.gamma\n",
        "epsilon = 5e-2\n",
        "\n",
        "# Prepare v, and storage\n",
        "v = np.zeros(env.Ns)\n",
        "v_all = []\n",
        "\n",
        "pi_all = []\n",
        "\n",
        "# iterate over the value\n",
        "while True:\n",
        "  \n",
        "  # Compute v_k\n",
        "  v, v_old = np.max(R + gamma * P.dot(v), axis=1), v\n",
        "  v_all.append(v)\n",
        "\n",
        "  # Esimate Intermediate policy\n",
        "  q = R + gamma * P.dot(v)\n",
        "  dpi = q.argmax(axis=1)\n",
        "\n",
        "  pi_all.append(dpi)\n",
        "\n",
        "  # stopping criterion \n",
        "  if np.absolute(v-v_old).max() < epsilon: \n",
        "    break\n",
        "\n",
        "# Plot optimal policy\n",
        "env.render_policy(dpi)\n",
        "\n",
        "# You need to evaluate V* here from your optimal policy\n",
        "# You have no guarantee that the last v-value is converge to the actual Vpi\n",
        "pi = sparsify_policy(dpi, Na=env.Na)\n",
        "Ppi = np.sum(P * np.expand_dims(pi, axis=-1), axis=1)\n",
        "Rpi = np.sum(R * pi, axis=1)\n",
        "\n",
        "v_star = np.linalg.inv( np.identity(env.Ns) - gamma*Ppi).dot(Rpi)\n",
        "print(\"v (last):\", v_all[-1])\n",
        "print(\"v (star):\", v_star)\n",
        "\n",
        "# The difference is not exactly epsilon. We here we are comparing V*-V while the algorithm we are comparing V_new - V_old\n",
        "print(\"epsilon: \", epsilon)\n",
        "print(\"|v - v_star|_inf: \", np.absolute(v_all[-1]-v_star).max())\n",
        "\n",
        "# The value function converge towards v* iteration after iteration\n",
        "plot_infnorm(v_all, v_star, name=\"V\")\n",
        "\n",
        "# If we estimate the intermediate policies, we observe that the optimal policy is obtained earlier in the process.\n",
        "# However, the value function has not yet converge. Thus ares still unaware that it is the optimal policty at iteration 4.\n",
        "plot_infnorm(pi_all, dpi, name=\"Pi\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TDT4iyJnAPIE"
      },
      "source": [
        "## Exercice: Policy Iteration (Homework)\n",
        "1. Implement Policy iteration!\n",
        "2. Evaluate the convergence of your estimate, i.e., plot the value $\\|V_n - V^\\star\\|_{\\infty}$\n",
        "3. Evaluate the convergence of your estimate, i.e., plot the value $\\|\\pi_n - \\pi^\\star\\|_{\\infty}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fpBCGnqNE7l1",
        "colab": {}
      },
      "source": [
        "# Retrieve the environment MDP\n",
        "P = env.P\n",
        "R = env.R\n",
        "gamma = env.gamma\n",
        "epsilon = 5e-2\n",
        "\n",
        "#Initialize policy\n",
        "dpi = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n",
        "print(env.render_policy(dpi))\n",
        "\n",
        "dpi = sparsify_policy(dpi, env.Na)\n",
        "\n",
        "v_all = []\n",
        "pi_all = []\n",
        "\n",
        "epsilon = 1e-2\n",
        "\n",
        "R_max = np.max(R)\n",
        "stop = int((np.log(R_max/(epsilon)))/(np.log(1/gamma))) + 1\n",
        "\n",
        "V_pi = np.zeros((env.Ns)) #init\n",
        "\n",
        "while True:\n",
        "\n",
        "  Ppi = np.sum(P * np.expand_dims(dpi, axis = -1), axis = 1)\n",
        "  Rpi = np.sum(R * dpi, axis = 1)\n",
        "\n",
        "  for i in range(stop):\n",
        "    V_pi = Rpi + gamma * Ppi.dot(V_pi)\n",
        "  \n",
        "  v_all.append(V_pi)\n",
        "\n",
        "  pi_old = np.argmax(dpi, axis = 1)\n",
        "\n",
        "  Q_pi = R + gamma * P.dot(V_pi)\n",
        "  pi = np.argmax(Q_pi, axis = 1)\n",
        "  pi_all.append(pi)\n",
        "\n",
        "  dpi = sparsify_policy(pi, env.Na)\n",
        "  \n",
        "\n",
        "  if np.abs(pi - pi_old).max() < epsilon : \n",
        "    break\n",
        "\n",
        "\n",
        "plot_infnorm(v_all, v_all[-1], name=\"V\")\n",
        "plot_infnorm(pi_all, pi_all[-1], name=\"Pi\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI3nihXQc6RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Retrieve the environment MDP\n",
        "P = env.P\n",
        "R = env.R\n",
        "gamma = env.gamma\n",
        "epsilon = 5e-2\n",
        "\n",
        "#Initialize policy\n",
        "dpi = np.zeros(shape=(env.Ns,), dtype=np.int32) + 2\n",
        "print(env.render_policy(dpi))\n",
        "v = np.zeros(env.Ns)\n",
        "v_all = []\n",
        "pi_all = []\n",
        "\n",
        "while True:\n",
        "\n",
        "  # Policy evaluation\n",
        "  v_all.append(v)\n",
        "  v = (R + gamma * np.dot(P, v)).max(axis=1)\n",
        "\n",
        "  # policy improvement\n",
        "  # Estimate Intermediate policy\n",
        "  pi_all.append(dpi)\n",
        "  q = R + gamma * P.dot(v)\n",
        "  dpi = q.argmax(axis=1)\n",
        "\n",
        "  if np.abs(pi_all[-1] -  dpi).max() <= epsilon: break\n",
        "\n",
        "print(env.render_policy(dpi))\n",
        "plot_infnorm(v_all, v, name=\"V\")\n",
        "plot_infnorm(pi_all, dpi, name=\"Pi\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WHrlauOwOX5L"
      },
      "source": [
        "## Exercice: Q learning\n",
        "Q learning is a model-free algorithm for estimating the optimal Q-function online.\n",
        "It is an off-policy algorithm since the samples are collected with a policy that is (potentially) not the one associated to the estimated Q-function.\n",
        "\n",
        "1. Implement Q learning with $\\epsilon$-greedy exploration.\n",
        "  - Plot the error in Q-functions over iterations\n",
        "  - Plot the cumulative sum of rewards"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QCxJruBNOX5M",
        "colab": {}
      },
      "source": [
        "# ---------------------------\n",
        "# Q-Learning\n",
        "# ---------------------------\n",
        "class QLearning:\n",
        "    \"\"\"\n",
        "    Q learning with epsilon-greedy exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate, epsilon, min_epsilon):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.Q = np.zeros((env.Ns, env.Na))\n",
        "        self.Nsa = np.ones((env.Ns, env.Na))\n",
        "    \n",
        "    def sample_action(self, state, greedy=False):\n",
        "        epsilon = min(\n",
        "            self.epsilon / math.sqrt(self.Nsa[state].min()),\n",
        "            self.min_epsilon)\n",
        "        \n",
        "        # explore\n",
        "        if not greedy and np.random.uniform(0, 1) < epsilon:\n",
        "          return np.random.choice(self.env.actions)\n",
        "\n",
        "        # exploit\n",
        "        else:\n",
        "          action = self.Q[state, :].argmax()\n",
        "          return self.env.actions[action]\n",
        "    \n",
        "    def update(self, state, action, next_state, reward, done):\n",
        "        alpha = self.learning_rate / math.sqrt(self.Nsa[state, action])\n",
        "        \n",
        "        if not done:\n",
        "          max_q = self.Q[next_state, :].max()\n",
        "        else:\n",
        "          max_q = 0.  # We do not bootstrap further\n",
        "        \n",
        "        q = self.Q[state, action]\n",
        "        increment = (reward + self.gamma*max_q) - q\n",
        "\n",
        "        # Update\n",
        "        self.Q[state, action] = self.Q[state, action] + alpha*increment\n",
        "        self.Nsa[state, action] += 1    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf1wjm3QcC5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install ipdb\n",
        "# import ipdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jsmojgubLVL9",
        "colab": {}
      },
      "source": [
        "q_learning = QLearning(env, gamma=env.gamma, learning_rate=1, epsilon=0.6, min_epsilon=0.1)\n",
        "\n",
        "# Define storage and variable\n",
        "q_all = []\n",
        "r_all = []\n",
        "pi_all = []\n",
        "max_steps = int(5e4)\n",
        "\n",
        "\n",
        "# main algorithmic loop\n",
        "state = env.reset()\n",
        "for t in range(max_steps):\n",
        "    \n",
        "  # Sample the action\n",
        "  action = q_learning.sample_action(state, greedy=False)\n",
        "  \n",
        "  # Sample the environment\n",
        "  next_state, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # Update q-function\n",
        "  q_learning.update(state=state, action=action, next_state=next_state, reward=reward, done=done)\n",
        "\n",
        "  # Store information \n",
        "  r_all.append(reward)\n",
        "  q_all.append(q_learning.Q)\n",
        "  pi_all.append(q_learning.Q.argmax(axis=1))\n",
        "  \n",
        "  state = next_state\n",
        "  if done:\n",
        "    state = env.reset()\n",
        "\n",
        "dpi = q_learning.Q.argmax(axis=1)\n",
        "print(env.render_policy(dpi))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ysm4Cq_pxj5T",
        "colab": {}
      },
      "source": [
        "state = env.reset()\n",
        "env.render()\n",
        "for i in range(5):\n",
        "    action = q_learning.sample_action(state, greedy=True)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-QyNzQpLIpAv",
        "colab": {}
      },
      "source": [
        "print(q_learning.Q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bi2j8vI4It8h",
        "colab": {}
      },
      "source": [
        "plot_infnorm(pi_all, pi_all[-1], name=\"Pi\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xViQ6kpzJfw9",
        "colab": {}
      },
      "source": [
        "pi = sparsify_policy(pi_all[-1], Na=env.Na)\n",
        "Ppi = np.sum(env.P * np.expand_dims(pi, axis=-1), axis=1)\n",
        "Rpi = np.sum(env.R * pi, axis=1)\n",
        "\n",
        "v_star = np.linalg.inv( np.identity(env.Ns) - gamma*Ppi).dot(Rpi)\n",
        "\n",
        "v_all = []\n",
        "for dpi_prev, q_prev in zip(pi_all, q_all):\n",
        "  pi_prev = sparsify_policy(dpi_prev, Na=env.Na)\n",
        "  v_prev = np.sum(pi_prev*q_prev, axis=1)\n",
        "  v_all.append(v_prev)\n",
        "\n",
        "plot_infnorm(v_all, v_star, name=\"V\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "59xsIjpIOX5P"
      },
      "source": [
        "## Exercice: SARSA (Homework)\n",
        "SARSA is another control algorithm. While Qlearning is off-policy, SARSA is on-policy.\n",
        "\n",
        "1. Implement SARSA with softmax (Gibbs) exploration and test the convergence to $Q^\\star$\n",
        "2. Plot the value $\\|V_n - V^\\star\\|_{\\infty}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gySnHtrsOX5Q",
        "colab": {}
      },
      "source": [
        "# ---------------------------\n",
        "# SARSA\n",
        "# ---------------------------\n",
        "class SARSA:\n",
        "    \"\"\"\n",
        "    SARSA with deacreasing epsilon for exploration\n",
        "    \"\"\"\n",
        "    def __init__(self, env, gamma, learning_rate, epsilon):\n",
        "      # Start with a random policy\n",
        "      self.env = env\n",
        "      self.gamma = gamma\n",
        "      self.learning_rate = learning_rate\n",
        "      self.epsilon = epsilon\n",
        "      # self.min_epsilon = min_epsilon\n",
        "      self.Q = np.zeros((env.Ns, env.Na))\n",
        "      self.Nsa = np.ones((env.Ns, env.Na))\n",
        "    \n",
        "    def sample_action(self, state, greedy=False):\n",
        "      epsilon = self.epsilon / (math.sqrt(self.Nsa[state].min()) + 1)\n",
        "      if not greedy and np.random.uniform(0, 1) < epsilon:\n",
        "        return np.random.choice(self.env.actions)\n",
        "      else:\n",
        "        return self.env.actions[self.Q[state].argmax()]\n",
        "        \n",
        "    def update(self, state, action, next_state, next_action, reward):\n",
        "      lr = self.learning_rate/(np.sqrt(self.Nsa[state, action]))\n",
        "\n",
        "      if done:\n",
        "        self.Q[state, action] = self.Q[state, action] + lr*(reward - self.Q[state,action])\n",
        "      else:\n",
        "        self.Q[state, action]=  self.Q[state, action] + lr*(reward + self.gamma*self.Q[next_state, next_action] - self.Q[state,action])\n",
        "      \n",
        "      self.Nsa[state,action] = self.Nsa[state,action] + 1\n",
        "\n",
        "      # self.Nsa[state][action] += 1\n",
        "      # alpha = self.learning_rate / math.sqrt(self.Nsa[state, action])\n",
        "      # self.Q[state][action] +=  alpha * (reward + (self.gamma * self.Q[next_state][next_action]) - self.Q[state][action])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Be0d4nOBOX5R",
        "colab": {}
      },
      "source": [
        "sarsa = SARSA(env, gamma=env.gamma, learning_rate=1., epsilon=1.)\n",
        "\n",
        "# Define storage and variable\n",
        "q_all = []\n",
        "r_all = []\n",
        "pi_all = []\n",
        "max_steps = int(5e4)\n",
        "\n",
        "\n",
        "# main algorithmic loop\n",
        "state = env.reset()\n",
        "for t in range(max_steps):\n",
        "    \n",
        "  # Sample the action\n",
        "  action = sarsa.sample_action(state)\n",
        "  \n",
        "  # Sample the environment\n",
        "  next_state, reward, done, info = env.step(action)\n",
        "  next_action = sarsa.sample_action(next_state)\n",
        "  \n",
        "  # Update q-function\n",
        "  sarsa.update(state, action, next_state, next_action, reward)\n",
        "\n",
        "  # Store information \n",
        "  r_all.append(reward)\n",
        "  q_all.append(sarsa.Q.copy())\n",
        "  pi_all.append(sarsa.Q.argmax(axis=1))\n",
        "  \n",
        "  state = next_state\n",
        "  if done:\n",
        "    state = env.reset()\n",
        "\n",
        "dpi = sarsa.Q.argmax(axis=1)\n",
        "print(env.render_policy(dpi))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHWHLHfHegWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_infnorm(q_all, sarsa.Q, name=\"sarsa\")\n",
        "plot_infnorm(pi_all, dpi, name=\"Pi\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOh6wjLMetw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = env.reset()\n",
        "env.render()\n",
        "for i in range(5):\n",
        "    action = sarsa.sample_action(state, greedy=True)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwjQt26Ke8mS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_infnorm(v_all, v_star, name=\"V\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUxeNX7IvJ6x",
        "colab_type": "text"
      },
      "source": [
        "<h1>About the Authors:</h1> \n",
        "\n",
        "<a href=\"https://skabongo.github.io/\">Salomon Kabongo</a>, Master degree student at <a href=\"https://aims.ac.za/\">the African Master in Machine Intelligence (AMMI, Ghana)</a> his research focused on the use machine learning technique in the field of Natural Language Processing, learn more about him [here](https://skabongo.github.io/) or https://skabongo.github.io/.\n",
        "\n",
        "**References :** *Introduction to Reiforcement learning AMMI, course* by [Matteo Pirotta](https://teopir.github.io/), Research Scientist at Facebook AI Research, assisted by [Floian Strub](https://fstrub95.github.io/) and [Omar Darwiche Domingue](https://omardrwch.github.io/).\n",
        "\n",
        "Copyright &copy; 2020. This notebook and its source code are released under the terms of the <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">Apache License 2.0</a>."
      ]
    }
  ]
}