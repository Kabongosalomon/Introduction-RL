{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "AMMI_EXPRL_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kabongosalomon/Introduction-RL/blob/master/Session%2004/AMMI_EXPRL_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UWM-gGi76pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5EbzJ1A78Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './mvarl_hands_on/exploration')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDPYhYvR7yKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from riverswim import RiverSwim\n",
        "import cvxpy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5twn7Qv7yKf",
        "colab_type": "text"
      },
      "source": [
        "# Finite-Horizon MDPs\n",
        "We consider finite horizon problems with horizon $H$. For simplicity, we consider MDPs with stationary transitions and rewards, ie these functions do not depend on the stage ($p_h =p$, $r_h=r$ for any $h \\in [H]$).\n",
        "\n",
        "The value of a policy or the optimal value function can be computed using *backward induction*.\n",
        "\n",
        "\n",
        "Given a deterministic (non-stationary) policy $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_H)$, backward induction applies the Bellman operator defined as\n",
        "$$\n",
        "V_h^\\pi(s) = \\sum_{s'} p(s'|s,\\pi_h(s)) \\left( r(s,\\pi_h(s),s') + V_{h+1}^\\pi(s')\\right)\n",
        "$$\n",
        "where $V_{H+1}(s) = 0$, for any $s$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l8433mz7yKh",
        "colab_type": "code",
        "outputId": "8b43c2d0-e079-485b-e0f0-da1425120711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "env = RiverSwim(6)\n",
        "H = 10\n",
        "print(\"Reward matrix: \", env.R.shape)\n",
        "print(env.R)\n",
        "print()\n",
        "print(\"Transition matrix: \", env.P.shape)\n",
        "print(\"Transitions probabilities for state s_1:\")\n",
        "print(env.P[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward matrix:  (6, 2)\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n",
            "\n",
            "Transition matrix:  (6, 2, 6)\n",
            "Transitions probabilities for state s_1:\n",
            "[[1.   0.   0.   0.   0.   0.  ]\n",
            " [0.05 0.6  0.35 0.   0.   0.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MViy1iO81o1g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31e542db-5a7b-451c-d06e-c1308fa353bd"
      },
      "source": [
        "print(env.P[0, 1, 1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXHdYle7yKm",
        "colab_type": "text"
      },
      "source": [
        "# Backward induction (aka Value Iteration)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_KZXn8A7yKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_induction(P, R, H):\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A)-dim matrix\n",
        "            H: horizon\n",
        "\n",
        "        Returns:\n",
        "            The optimal V-function\n",
        "            The optimal policy\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    policy = np.zeros((H, S), dtype=np.int)\n",
        "    V = np.zeros((H + 1, S))\n",
        "    for h in reversed(range(H)):\n",
        "        for s in range(S):\n",
        "            \"\"\" \n",
        "            Here, we compute V^*(h, s) using the Bellman optimality equation:\n",
        "\n",
        "            V[h, s] = max_a  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n",
        "            \"\"\"\n",
        "            for a in range(A):\n",
        "                tmp = R[s, a] + np.dot(P[s, a],  V[h + 1])\n",
        "                if (a == 0) or (tmp > V[h, s]):\n",
        "                    policy[h, s] = a\n",
        "                    V[h, s] = tmp\n",
        "    return V, policy\n",
        "\n",
        "def policy_evaluation(P, R, H, policy):\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A)-dim matrix\n",
        "            H: horizon\n",
        "            policy: policy (H,S)-dim matrix\n",
        "\n",
        "        Returns:\n",
        "            The V-function of the given policy\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    V = np.zeros((H + 1, S))\n",
        "    for h in reversed(range(H)):\n",
        "        for s in range(S):\n",
        "            \"\"\" \n",
        "            Here, we compute V^pi(h, s) using the Bellman equation for the policy pi:\n",
        "\n",
        "            a = policy[h,s]\n",
        "            V[h, s] =  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n",
        "            \"\"\"\n",
        "            a = policy[h,s]\n",
        "            # complete the policy evalution here\n",
        "            V[h, s] =  R[s, a] + sum([P[s, a, s_]*V[h+1, s_] for s_ in range(S)])\n",
        "\n",
        "    return V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvmhRCN07yKs",
        "colab_type": "text"
      },
      "source": [
        "Compute solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVjkydge7yKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "b2071b19-00f7-4879-ec32-5a45a4b2eefe"
      },
      "source": [
        "Vstar, POLstar = backward_induction(env.P, env.R, H)\n",
        "\n",
        "print(\"Optimal policy\")\n",
        "print(np.round(Vstar))\n",
        "\n",
        "print(POLstar)\n",
        "\n",
        "V_policy = policy_evaluation(env.P, env.R, H, POLstar)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal policy\n",
            "[[0. 1. 1. 2. 4. 5.]\n",
            " [0. 0. 1. 2. 3. 5.]\n",
            " [0. 0. 1. 2. 3. 4.]\n",
            " [0. 0. 1. 1. 3. 4.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 0. 1. 3.]\n",
            " [0. 0. 0. 0. 1. 2.]\n",
            " [0. 0. 0. 0. 0. 2.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "[[1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [0 1 1 1 1 1]\n",
            " [0 1 1 1 1 1]\n",
            " [0 0 1 1 1 1]\n",
            " [0 0 0 1 1 1]\n",
            " [0 0 0 0 1 1]\n",
            " [0 0 0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMmQ4Gdl4L6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "ee8a3a85-14b2-491b-d679-9fa272a7660a"
      },
      "source": [
        "# To test your implementation:\n",
        "V_policy = policy_evaluation(env.P, env.R, H, POLstar)\n",
        "# V_policy must be equal to Vstar\n",
        "print(\"difference V_policy - Vstar\")\n",
        "print(np.abs(V_policy - Vstar).sum())\n",
        "print(np.round(V_policy))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "difference V_policy - Vstar\n",
            "0.0\n",
            "[[0. 1. 1. 2. 4. 5.]\n",
            " [0. 0. 1. 2. 3. 5.]\n",
            " [0. 0. 1. 2. 3. 4.]\n",
            " [0. 0. 1. 1. 3. 4.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 0. 1. 3.]\n",
            " [0. 0. 0. 0. 1. 2.]\n",
            " [0. 0. 0. 0. 0. 2.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Joy0Rb9194A1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c248d7fa-06a1-4b0d-d823-e3b4de0188e2"
      },
      "source": [
        "S, A = env.R.shape   # number of states and actions\n",
        "Phat = np.zeros((S, A, S))\n",
        "Rhat = np.zeros((S, A))\n",
        "\n",
        "N_sa = np.zeros((S, A)) # number of visits to each state-action\n",
        "N_sas = np.zeros((S, A, S))\n",
        "S_sa = np.zeros((S, A)) # sum of rewards obtained when visiting each\n",
        "                        # state, action\n",
        "\n",
        "# Interact with the environment to estimate P and R\n",
        "\n",
        "nb_episodes = 200\n",
        "# Loop over episodes\n",
        "for ep in range(nb_episodes):\n",
        "    state = env.reset()\n",
        "    for h in range(H):\n",
        "        action = np.random.choice(A)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Update estimates\n",
        "        N_sa[state, action] += 1\n",
        "        N_sas[state, action, next_state] +=1\n",
        "        S_sa[state, action] += reward\n",
        "        \n",
        "        Rhat[state, action] = S_sa[state, action]/N_sa[state, action]\n",
        "        Phat[state, action, :] = N_sas[state, action, :]/N_sa[state, action]\n",
        "\n",
        "        state = next_state \n",
        "\n",
        "\n",
        "print(Rhat)\n",
        "print(env.R)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIGVqAVLC0b1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "8bf14b72-091b-4ee5-bedb-ac4a9f0d0f7a"
      },
      "source": [
        "# Check confidence interval for rewards\n",
        "#  - With high probability, we have\n",
        "#    R(s, a) is in the interval\n",
        "#            [R_hat[s,a] - beta_r[s,a], R_hat[s,a] + beta_r[s,a]]\n",
        "#\n",
        "#  where beta_r[s, a] = sqrt(log(S*A*N[s,a])/N[s,a] )\n",
        "# \n",
        "beta_r = np.zeros((S, A))\n",
        "for s in range(S):\n",
        "    for a in range(A):\n",
        "        n = max(N_sa[s,a], 1)\n",
        "        beta_r[s, a] = np.sqrt(np.log(S*A*n)/n )\n",
        "\n",
        "print(\"\")\n",
        "print(Rhat)\n",
        "print(Rhat + beta_r)\n",
        "print(Rhat - beta_r)\n",
        "print(\"True R\")\n",
        "print(env.R)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n",
            "[[0.1252551  0.11639171]\n",
            " [0.170558   0.1727467 ]\n",
            " [0.36389409 0.32007917]\n",
            " [0.57323072 0.44922164]\n",
            " [0.84426162 0.90491376]\n",
            " [1.57635867 2.57635867]]\n",
            "[[-0.1152551  -0.11639171]\n",
            " [-0.170558   -0.1727467 ]\n",
            " [-0.36389409 -0.32007917]\n",
            " [-0.57323072 -0.44922164]\n",
            " [-0.84426162 -0.90491376]\n",
            " [-1.57635867 -0.57635867]]\n",
            "True R\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1nCGWuw7yKy",
        "colab_type": "text"
      },
      "source": [
        "## UCRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoWtsh7v7yK0",
        "colab_type": "text"
      },
      "source": [
        "UCRL is an algorithm for efficient exploration in finite-horizon tabular MDP.\n",
        "In this setting, the regret is defined as\n",
        "$$R(K) = \\sum_{k=1}^K V^\\star_1(s_{k,1}) - V^{\\pi_k}_1(s_{k,1})$$\n",
        "UCBVI enjoys a regret bound of order $O(\\sqrt{HSAK})$.\n",
        "\n",
        "The structure of the algorithm is as follow\n",
        "\n",
        "For $k = 1, \\ldots, K$ do<br>\n",
        "> Solve optimistic planning problem -> $(V_k, Q_k, \\pi_k)$<br>\n",
        "> Execute the optimistic policy $\\pi_k$ for $H$ steps<br>\n",
        ">> for $h=1, \\ldots, H$<br>\n",
        ">>> $a_{k,h} = \\pi(s_{k,h})$<br>\n",
        ">>> execute $a_{k,h}$, observe $r_{k,h}$ and $s_{k, h+1}$<br>\n",
        ">>> $N(s_{k,h}, a_{k,h}, s_{k,h+1}) += 1$ (update also estimated reward and transitions)\n",
        "\n",
        "<font color='#ed7d31'>Optimistic planning</font>\n",
        "At each episode, UCRL computes the optimal solution by solving the following \"extended\" problem\n",
        "$$\n",
        "V_h^\\star(s) =  \\max_{r \\in B_r(s,a)} r + \\max_{p \\in B_p(s,a)} \\sum_{s'} p(s') V_{h+1}(s') \n",
        "$$\n",
        "where $V_{H+1}(s) = 0$ and $B_r(s,a)$ and $B_p(s,a)$ are confidence intervals on the estimated transitions and rewards:\n",
        "\n",
        "$$\n",
        "B_r(s,a) = \\{ r(s,a):  |r_s, a) - \\hat{r}(s,a)| \\leq \\beta_r(s,a)  \\}\n",
        "$$\n",
        "\n",
        "$$\n",
        "B_p(s,a) = \\{ p(\\cdot|s,a):  ||p(\\cdot|s,a) - \\hat{p}(\\cdot|s,a)||_1 \\leq \\beta_p(s,a)  \\}\n",
        "$$\n",
        "where \n",
        "\n",
        "$$\n",
        "\\beta_r(s, a) = \\sqrt{ \\frac{ \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  } \\\\\n",
        "\\beta_p(s, a) = \\sqrt{ \\frac{  S \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  }\n",
        "$$\n",
        "and where  $N^+(s, a) = \\max(1, N(s, a))$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji7IShLN7yK1",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "The following function computes:\n",
        "$$\\max_{x \\in B_p} \\sum_{s'} x(s') V(s') $$\n",
        "where $B_p = [P-\\beta, P+\\beta]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57zVp32L7yK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LPprobaH(v, P, beta, verbose=0):\n",
        "    \"\"\"\n",
        "        max_x v^T x\n",
        "        s.t.    0 <= x_i <= 1\n",
        "                \\sum_i |x_i - p_i| \\leq beta\n",
        "                \\sum_i x_i = 1\n",
        "    \"\"\"\n",
        "    sorted_idxs = np.argsort(v)[::-1]\n",
        "\n",
        "    pest = P.copy()\n",
        "    idx = sorted_idxs[0]\n",
        "    pest[idx] = min(1., P[idx] + beta / 2.)\n",
        "    delta = pest.sum()\n",
        "    j = len(P)-1\n",
        "    while delta > 1:\n",
        "        idx_j = sorted_idxs[j]\n",
        "        m = max(0, 1. - delta + pest[idx_j])\n",
        "        delta = delta - pest[idx_j] + m\n",
        "        pest[idx_j] = m\n",
        "        j -= 1\n",
        "    w = np.dot(pest, v)\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk0v7jg67yK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def UCRL(mdp, H, nb_episodes, VSTAR=0):\n",
        "    S, A = mdp.Ns, mdp.Na\n",
        "    policy = np.zeros((H, S), dtype=np.int)\n",
        "    Phat = np.ones((S,A,S)) / S\n",
        "    Rhat = np.zeros((S,A))\n",
        "    N_sas = np.zeros((S,A,S), dtype=np.int)\n",
        "    N_sa = np.zeros((S,A), dtype=np.int)\n",
        "    regret = np.zeros((nb_episodes,))\n",
        "    V = np.zeros((H + 1, S))\n",
        "    temp = np.zeros(A)\n",
        "    \n",
        "    delta = 0.1\n",
        "    \n",
        "    for k in range(nb_episodes):\n",
        "        \n",
        "        # compute optimistic solution\n",
        "        # 1. compute confidence intervals\n",
        "        N = np.maximum(N_sa, 1)\n",
        "        LOGT = np.log(S * A * N / delta)\n",
        "        beta_r = LOGT\n",
        "        beta_p = S*LOGT\n",
        "        \n",
        "        # 2. run extended value iteration\n",
        "        V.fill(0)\n",
        "        for h in reversed(range(H)):\n",
        "            for s in range(S):\n",
        "                for a in range(A):\n",
        "                    dotp = LPprobaH(V[h + 1], Phat[s, a], beta_p[s, a])\n",
        "                    # V[h, s] = max_a  Rhat[s, a] + beta_r[s,a] + ...\n",
        "                    temp[a] = Rhat[s, a]+ beta_r[s,a] + dotp\n",
        "\n",
        "                V[h, s] = np.max([temp[a_p] for a_p in range(A)])\n",
        "        \n",
        "        # execute policy\n",
        "        initial_state = state = mdp.reset()\n",
        "        for h in range(H):\n",
        "            action = policy[h][state]\n",
        "            next_state, reward, done, _ = mdp.step(action)\n",
        "            \n",
        "            # update estimates (Phat, Rhat, N_sa, N_sas)\n",
        "\n",
        "            N_sa[state, action] += 1\n",
        "            N_sas[state, action, next_state] +=1\n",
        "            S_sa[state, action] += reward\n",
        "            \n",
        "            Rhat[state, action] = S_sa[state, action]/N_sa[state, action]\n",
        "            Phat[state, action, :] = N_sas[state, action, :]/N_sa[state, action]\n",
        "\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # update regret\n",
        "        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n",
        "        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n",
        "        \n",
        "        if k % 50 == 0:\n",
        "            print(\"regret[{}]: {}\".format(k, regret[k]))\n",
        "    return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-UTr3Gq7yK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "85d14562-155c-4397-b7ac-cf3a30c999a3"
      },
      "source": [
        "nb_repetitions = 5\n",
        "nb_episodes = 750\n",
        "regrets = np.zeros((nb_repetitions, nb_episodes))\n",
        "for it in range(nb_repetitions):\n",
        "    print(\"Running simulation: {}\".format(it))\n",
        "    regrets[it] = UCRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running simulation: 0\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3023839779999999\n",
            "regret[100]: 0.3023839779999999\n",
            "regret[150]: 0.3023839779999999\n",
            "regret[200]: 0.3023839779999999\n",
            "regret[250]: 0.3023839779999999\n",
            "regret[300]: 0.3023839779999999\n",
            "regret[350]: 0.3023839779999999\n",
            "regret[400]: 0.3023839779999999\n",
            "regret[450]: 0.3023839779999999\n",
            "regret[500]: 0.3023839779999999\n",
            "regret[550]: 0.3023839779999999\n",
            "regret[600]: 0.3023839779999999\n",
            "regret[650]: 0.3023839779999999\n",
            "regret[700]: 0.3023839779999999\n",
            "Running simulation: 1\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3023839779999999\n",
            "regret[100]: 0.3023839779999999\n",
            "regret[150]: 0.3023839779999999\n",
            "regret[200]: 0.3023839779999999\n",
            "regret[250]: 0.3023839779999999\n",
            "regret[300]: 0.3023839779999999\n",
            "regret[350]: 0.3023839779999999\n",
            "regret[400]: 0.3023839779999999\n",
            "regret[450]: 0.3023839779999999\n",
            "regret[500]: 0.3023839779999999\n",
            "regret[550]: 0.3023839779999999\n",
            "regret[600]: 0.3023839779999999\n",
            "regret[650]: 0.3023839779999999\n",
            "regret[700]: 0.3023839779999999\n",
            "Running simulation: 2\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3023839779999999\n",
            "regret[100]: 0.3023839779999999\n",
            "regret[150]: 0.3023839779999999\n",
            "regret[200]: 0.3023839779999999\n",
            "regret[250]: 0.3023839779999999\n",
            "regret[300]: 0.3023839779999999\n",
            "regret[350]: 0.3023839779999999\n",
            "regret[400]: 0.3023839779999999\n",
            "regret[450]: 0.3023839779999999\n",
            "regret[500]: 0.3023839779999999\n",
            "regret[550]: 0.3023839779999999\n",
            "regret[600]: 0.3023839779999999\n",
            "regret[650]: 0.3023839779999999\n",
            "regret[700]: 0.3023839779999999\n",
            "Running simulation: 3\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3023839779999999\n",
            "regret[100]: 0.3023839779999999\n",
            "regret[150]: 0.3023839779999999\n",
            "regret[200]: 0.3023839779999999\n",
            "regret[250]: 0.3023839779999999\n",
            "regret[300]: 0.3023839779999999\n",
            "regret[350]: 0.3023839779999999\n",
            "regret[400]: 0.3023839779999999\n",
            "regret[450]: 0.3023839779999999\n",
            "regret[500]: 0.3023839779999999\n",
            "regret[550]: 0.3023839779999999\n",
            "regret[600]: 0.3023839779999999\n",
            "regret[650]: 0.3023839779999999\n",
            "regret[700]: 0.3023839779999999\n",
            "Running simulation: 4\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3023839779999999\n",
            "regret[100]: 0.3023839779999999\n",
            "regret[150]: 0.3023839779999999\n",
            "regret[200]: 0.3023839779999999\n",
            "regret[250]: 0.3023839779999999\n",
            "regret[300]: 0.3023839779999999\n",
            "regret[350]: 0.3023839779999999\n",
            "regret[400]: 0.3023839779999999\n",
            "regret[450]: 0.3023839779999999\n",
            "regret[500]: 0.3023839779999999\n",
            "regret[550]: 0.3023839779999999\n",
            "regret[600]: 0.3023839779999999\n",
            "regret[650]: 0.3023839779999999\n",
            "regret[700]: 0.3023839779999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M49yfZ_S7yLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a4b77736-d86d-4d55-ad36-0ce8f0122cfd"
      },
      "source": [
        "x = regrets.cumsum(axis=-1)\n",
        "mean_regret = x.mean(axis=0)\n",
        "std_regret = x.std(axis=0)\n",
        "plt.plot(mean_regret)\n",
        "plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n",
        "plt.ylabel('regret')\n",
        "\n",
        "# SAVE PSRL REGRET\n",
        "ucrl_regret = mean_regret"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIhX+P09gRj4HUZfkAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myPuG9Er7yLL",
        "colab_type": "text"
      },
      "source": [
        "# Posterior Sampling for RL\n",
        "\n",
        "At each iteration, PSRL samples one MDP from the posterior distribution and run the associated optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gxP6B9r7yLN",
        "colab_type": "text"
      },
      "source": [
        "Implement posterior sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl9YwtfJ7yLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PSRL(mdp, H, nb_episodes, VSTAR=0):\n",
        "    reward_prior = [1,1]\n",
        "    S, A = mdp.Ns, mdp.Na\n",
        "    Phat = np.ones((S,A,S)) / S\n",
        "    Rhat = np.zeros((S,A))\n",
        "    N_sas = np.zeros((S,A,S), dtype=np.int)\n",
        "    N_sa = np.zeros((S,A), dtype=np.int)\n",
        "    regret = np.zeros((nb_episodes,))\n",
        "    \n",
        "    for k in range(nb_episodes):\n",
        "        \n",
        "        # compute policy\n",
        "        # 1. sample MDP\n",
        "        R = np.zeros_like(Rhat)\n",
        "        P = np.zeros((S, A, S))\n",
        "        for s in range(S):\n",
        "            for a in range(A):\n",
        "                # sample transition matrix\n",
        "                # P[s, a] follows a dirichlet Dirichlet distribution of parameters N_sas[s, a,:] + 1\n",
        "                P[s, a] = ...\n",
        "\n",
        "                # posterior for Bernoulli rewards\n",
        "                N = N_sa[s, a]\n",
        "                v = N * Rhat[s, a]\n",
        "                a0 = reward_prior[0] + v\n",
        "                b0 = reward_prior[1] + N - v\n",
        "                p = np.random.beta(a=a0, b=b0, size=1).item()\n",
        "                R[s, a] = p\n",
        "        \n",
        "        # 2. compute optimal policy\n",
        "        V, policy = ...\n",
        "        \n",
        "        # execute policy\n",
        "        initial_state = state = mdp.reset()\n",
        "        for h in range(H):\n",
        "            action = policy[h][state]\n",
        "            next_state, reward, done, _ = mdp.step(action)\n",
        "            \n",
        "            # update estimates (Rhat, N_sa, N_sas)\n",
        "            ...\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # update regret\n",
        "        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n",
        "        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n",
        "        \n",
        "        if k % 50 == 0:\n",
        "            print(\"regret[{}]: {}\".format(k, regret[k]))\n",
        "\n",
        "    return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT38aVOB7yLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_repetitions = 5\n",
        "nb_episodes = 750\n",
        "regrets = np.zeros((nb_repetitions, nb_episodes))\n",
        "for it in range(nb_repetitions):\n",
        "    print(\"Running simulation: {}\".format(it))\n",
        "    regrets[it] = PSRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQhP7cpn7yLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = regrets.cumsum(axis=-1)\n",
        "mean_regret = x.mean(axis=0)\n",
        "std_regret = x.std(axis=0)\n",
        "plt.plot(mean_regret)\n",
        "plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n",
        "plt.ylabel('regret')\n",
        "\n",
        "# SAVE PSRL REGRET\n",
        "psrl_regret = mean_regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phHZWwdJ7yLc",
        "colab_type": "text"
      },
      "source": [
        "Compare algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvcqwaJY7yLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(ucrl_regret, label='UCRL-H')\n",
        "plt.plot(psrl_regret, label='PSRL')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it2tQtxL7yLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}