{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "AMMI_EXPRL_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kabongosalomon/Introduction-RL/blob/master/Session%2004/AMMI_EXPRL_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UWM-gGi76pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5EbzJ1A78Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './mvarl_hands_on/exploration')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDPYhYvR7yKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from riverswim import RiverSwim\n",
        "import cvxpy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5twn7Qv7yKf",
        "colab_type": "text"
      },
      "source": [
        "# Finite-Horizon MDPs\n",
        "We consider finite horizon problems with horizon $H$. For simplicity, we consider MDPs with stationary transitions and rewards, ie these functions do not depend on the stage ($p_h =p$, $r_h=r$ for any $h \\in [H]$).\n",
        "\n",
        "The value of a policy or the optimal value function can be computed using *backward induction*.\n",
        "\n",
        "\n",
        "Given a deterministic (non-stationary) policy $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_H)$, backward induction applies the Bellman operator defined as\n",
        "$$\n",
        "V_h^\\pi(s) = \\sum_{s'} p(s'|s,\\pi_h(s)) \\left( r(s,\\pi_h(s),s') + V_{h+1}^\\pi(s')\\right)\n",
        "$$\n",
        "where $V_{H+1}(s) = 0$, for any $s$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l8433mz7yKh",
        "colab_type": "code",
        "outputId": "8b43c2d0-e079-485b-e0f0-da1425120711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "env = RiverSwim(6)\n",
        "H = 10\n",
        "print(\"Reward matrix: \", env.R.shape)\n",
        "print(env.R)\n",
        "print()\n",
        "print(\"Transition matrix: \", env.P.shape)\n",
        "print(\"Transitions probabilities for state s_1:\")\n",
        "print(env.P[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward matrix:  (6, 2)\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n",
            "\n",
            "Transition matrix:  (6, 2, 6)\n",
            "Transitions probabilities for state s_1:\n",
            "[[1.   0.   0.   0.   0.   0.  ]\n",
            " [0.05 0.6  0.35 0.   0.   0.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MViy1iO81o1g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "31e542db-5a7b-451c-d06e-c1308fa353bd"
      },
      "source": [
        "print(env.P[0, 1, 1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXHdYle7yKm",
        "colab_type": "text"
      },
      "source": [
        "# Backward induction (aka Value Iteration)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_KZXn8A7yKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_induction(P, R, H):\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A)-dim matrix\n",
        "            H: horizon\n",
        "\n",
        "        Returns:\n",
        "            The optimal V-function\n",
        "            The optimal policy\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    policy = np.zeros((H, S), dtype=np.int)\n",
        "    V = np.zeros((H + 1, S))\n",
        "    for h in reversed(range(H)):\n",
        "        for s in range(S):\n",
        "            \"\"\" \n",
        "            Here, we compute V^*(h, s) using the Bellman optimality equation:\n",
        "\n",
        "            V[h, s] = max_a  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n",
        "            \"\"\"\n",
        "            for a in range(A):\n",
        "                tmp = R[s, a] + np.dot(P[s, a],  V[h + 1])\n",
        "                if (a == 0) or (tmp > V[h, s]):\n",
        "                    policy[h, s] = a\n",
        "                    V[h, s] = tmp\n",
        "    return V, policy\n",
        "\n",
        "def policy_evaluation(P, R, H, policy):\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A)-dim matrix\n",
        "            H: horizon\n",
        "            policy: policy (H,S)-dim matrix\n",
        "\n",
        "        Returns:\n",
        "            The V-function of the given policy\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    V = np.zeros((H + 1, S))\n",
        "    for h in reversed(range(H)):\n",
        "        for s in range(S):\n",
        "            \"\"\" \n",
        "            Here, we compute V^pi(h, s) using the Bellman equation for the policy pi:\n",
        "\n",
        "            a = policy[h,s]\n",
        "            V[h, s] =  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n",
        "            \"\"\"\n",
        "            a = policy[h,s]\n",
        "            # complete the policy evalution here\n",
        "            V[h, s] =  R[s, a] + sum([P[s, a, s_]*V[h+1, s_] for s_ in range(S)])\n",
        "\n",
        "    return V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvmhRCN07yKs",
        "colab_type": "text"
      },
      "source": [
        "Compute solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVjkydge7yKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "b2071b19-00f7-4879-ec32-5a45a4b2eefe"
      },
      "source": [
        "Vstar, POLstar = backward_induction(env.P, env.R, H)\n",
        "\n",
        "print(\"Optimal policy\")\n",
        "print(np.round(Vstar))\n",
        "\n",
        "print(POLstar)\n",
        "\n",
        "V_policy = policy_evaluation(env.P, env.R, H, POLstar)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal policy\n",
            "[[0. 1. 1. 2. 4. 5.]\n",
            " [0. 0. 1. 2. 3. 5.]\n",
            " [0. 0. 1. 2. 3. 4.]\n",
            " [0. 0. 1. 1. 3. 4.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 0. 1. 3.]\n",
            " [0. 0. 0. 0. 1. 2.]\n",
            " [0. 0. 0. 0. 0. 2.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "[[1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [0 1 1 1 1 1]\n",
            " [0 1 1 1 1 1]\n",
            " [0 0 1 1 1 1]\n",
            " [0 0 0 1 1 1]\n",
            " [0 0 0 0 1 1]\n",
            " [0 0 0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMmQ4Gdl4L6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "ee8a3a85-14b2-491b-d679-9fa272a7660a"
      },
      "source": [
        "# To test your implementation:\n",
        "V_policy = policy_evaluation(env.P, env.R, H, POLstar)\n",
        "# V_policy must be equal to Vstar\n",
        "print(\"difference V_policy - Vstar\")\n",
        "print(np.abs(V_policy - Vstar).sum())\n",
        "print(np.round(V_policy))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "difference V_policy - Vstar\n",
            "0.0\n",
            "[[0. 1. 1. 2. 4. 5.]\n",
            " [0. 0. 1. 2. 3. 5.]\n",
            " [0. 0. 1. 2. 3. 4.]\n",
            " [0. 0. 1. 1. 3. 4.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 0. 1. 3.]\n",
            " [0. 0. 0. 0. 1. 2.]\n",
            " [0. 0. 0. 0. 0. 2.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Joy0Rb9194A1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c248d7fa-06a1-4b0d-d823-e3b4de0188e2"
      },
      "source": [
        "S, A = env.R.shape   # number of states and actions\n",
        "Phat = np.zeros((S, A, S))\n",
        "Rhat = np.zeros((S, A))\n",
        "\n",
        "N_sa = np.zeros((S, A)) # number of visits to each state-action\n",
        "N_sas = np.zeros((S, A, S))\n",
        "S_sa = np.zeros((S, A)) # sum of rewards obtained when visiting each\n",
        "                        # state, action\n",
        "\n",
        "# Interact with the environment to estimate P and R\n",
        "\n",
        "nb_episodes = 200\n",
        "# Loop over episodes\n",
        "for ep in range(nb_episodes):\n",
        "    state = env.reset()\n",
        "    for h in range(H):\n",
        "        action = np.random.choice(A)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Update estimates\n",
        "        N_sa[state, action] += 1\n",
        "        N_sas[state, action, next_state] +=1\n",
        "        S_sa[state, action] += reward\n",
        "        \n",
        "        Rhat[state, action] = S_sa[state, action]/N_sa[state, action]\n",
        "        Phat[state, action, :] = N_sas[state, action, :]/N_sa[state, action]\n",
        "\n",
        "        state = next_state \n",
        "\n",
        "\n",
        "print(Rhat)\n",
        "print(env.R)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIGVqAVLC0b1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "790cbfe5-0516-4f65-b568-4f4211f9add5"
      },
      "source": [
        "# Check confidence interval for rewards\n",
        "#  - With high probability, we have\n",
        "#    R(s, a) is in the interval\n",
        "#            [R_hat[s,a] - beta_r[s,a], R_hat[s,a] + beta_r[s,a]]\n",
        "#\n",
        "#  where beta_r[s, a] = sqrt(log(S*A*N[s,a])/N[s,a] )\n",
        "# \n",
        "beta_r = np.zeros((S, A))\n",
        "for s in range(S):\n",
        "    for a in range(A):\n",
        "        n = max(N_sa[s,a], 1)\n",
        "        beta_r[s, a] = np.sqrt(np.log(S*A*n)/n )\n",
        "\n",
        "def get(N_s, S, A):\n",
        "  delta = 0.1\n",
        "  beta_r = np.zeros((S, A))\n",
        "  beta_p = np.zeros((S, A))\n",
        "  for s in range(S):\n",
        "      for a in range(A):\n",
        "          n = max(N_sa[s,a], 1)\n",
        "          beta_r[s, a] = np.sqrt(np.log(S*A*n)/n )\n",
        "          beta_p[s, a] = np.sqrt(S*np.log(S*A*n)/n )\n",
        "  return beta_r, beta_p\n",
        "\n",
        "print(\"\")\n",
        "print(Rhat)\n",
        "print(Rhat + beta_r)\n",
        "print(Rhat - beta_r)\n",
        "print(\"True R\")\n",
        "print(env.R)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n",
            "[[0.1252551  0.11639171]\n",
            " [0.170558   0.1727467 ]\n",
            " [0.36389409 0.32007917]\n",
            " [0.57323072 0.44922164]\n",
            " [0.84426162 0.90491376]\n",
            " [1.57635867 2.57635867]]\n",
            "[[-0.1152551  -0.11639171]\n",
            " [-0.170558   -0.1727467 ]\n",
            " [-0.36389409 -0.32007917]\n",
            " [-0.57323072 -0.44922164]\n",
            " [-0.84426162 -0.90491376]\n",
            " [-1.57635867 -0.57635867]]\n",
            "True R\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1nCGWuw7yKy",
        "colab_type": "text"
      },
      "source": [
        "## UCRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoWtsh7v7yK0",
        "colab_type": "text"
      },
      "source": [
        "UCRL is an algorithm for efficient exploration in finite-horizon tabular MDP.\n",
        "In this setting, the regret is defined as\n",
        "$$R(K) = \\sum_{k=1}^K V^\\star_1(s_{k,1}) - V^{\\pi_k}_1(s_{k,1})$$\n",
        "UCBVI enjoys a regret bound of order $O(\\sqrt{HSAK})$.\n",
        "\n",
        "The structure of the algorithm is as follow\n",
        "\n",
        "For $k = 1, \\ldots, K$ do<br>\n",
        "> Solve optimistic planning problem -> $(V_k, Q_k, \\pi_k)$<br>\n",
        "> Execute the optimistic policy $\\pi_k$ for $H$ steps<br>\n",
        ">> for $h=1, \\ldots, H$<br>\n",
        ">>> $a_{k,h} = \\pi(s_{k,h})$<br>\n",
        ">>> execute $a_{k,h}$, observe $r_{k,h}$ and $s_{k, h+1}$<br>\n",
        ">>> $N(s_{k,h}, a_{k,h}, s_{k,h+1}) += 1$ (update also estimated reward and transitions)\n",
        "\n",
        "<font color='#ed7d31'>Optimistic planning</font>\n",
        "At each episode, UCRL computes the optimal solution by solving the following \"extended\" problem\n",
        "$$\n",
        "V_h^\\star(s) =  \\max_{r \\in B_r(s,a)} r + \\max_{p \\in B_p(s,a)} \\sum_{s'} p(s') V_{h+1}(s') \n",
        "$$\n",
        "where $V_{H+1}(s) = 0$ and $B_r(s,a)$ and $B_p(s,a)$ are confidence intervals on the estimated transitions and rewards:\n",
        "\n",
        "$$\n",
        "B_r(s,a) = \\{ r(s,a):  |r_s, a) - \\hat{r}(s,a)| \\leq \\beta_r(s,a)  \\}\n",
        "$$\n",
        "\n",
        "$$\n",
        "B_p(s,a) = \\{ p(\\cdot|s,a):  ||p(\\cdot|s,a) - \\hat{p}(\\cdot|s,a)||_1 \\leq \\beta_p(s,a)  \\}\n",
        "$$\n",
        "where \n",
        "\n",
        "$$\n",
        "\\beta_r(s, a) = \\sqrt{ \\frac{ \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  } \\\\\n",
        "\\beta_p(s, a) = \\sqrt{ \\frac{  S \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  }\n",
        "$$\n",
        "and where  $N^+(s, a) = \\max(1, N(s, a))$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji7IShLN7yK1",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "The following function computes:\n",
        "$$\\max_{x \\in B_p} \\sum_{s'} x(s') V(s') $$\n",
        "where $B_p = [P-\\beta, P+\\beta]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57zVp32L7yK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LPprobaH(v, P, beta, verbose=0):\n",
        "    \"\"\"\n",
        "        max_x v^T x\n",
        "        s.t.    0 <= x_i <= 1\n",
        "                \\sum_i |x_i - p_i| \\leq beta\n",
        "                \\sum_i x_i = 1\n",
        "    \"\"\"\n",
        "    sorted_idxs = np.argsort(v)[::-1]\n",
        "\n",
        "    pest = P.copy()\n",
        "    idx = sorted_idxs[0]\n",
        "    pest[idx] = min(1., P[idx] + beta / 2.)\n",
        "    delta = pest.sum()\n",
        "    j = len(P)-1\n",
        "    while delta > 1:\n",
        "        idx_j = sorted_idxs[j]\n",
        "        m = max(0, 1. - delta + pest[idx_j])\n",
        "        delta = delta - pest[idx_j] + m\n",
        "        pest[idx_j] = m\n",
        "        j -= 1\n",
        "    w = np.dot(pest, v)\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk0v7jg67yK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def UCRL(mdp, H, nb_episodes, VSTAR=0):\n",
        "    S, A = mdp.Ns, mdp.Na\n",
        "    policy = np.zeros((H, S), dtype=np.int)\n",
        "    Phat = np.ones((S,A,S)) / S\n",
        "    Rhat = np.zeros((S,A))\n",
        "    N_sas = np.zeros((S,A,S), dtype=np.int)\n",
        "    N_sa = np.zeros((S,A), dtype=np.int)\n",
        "    regret = np.zeros((nb_episodes,))\n",
        "    V = np.zeros((H + 1, S))\n",
        "    temp = np.zeros(A)\n",
        "    \n",
        "    delta = 0.1\n",
        "    \n",
        "    for k in range(nb_episodes):\n",
        "        \n",
        "        # compute optimistic solution\n",
        "        # 1. compute confidence intervals\n",
        "        N = np.maximum(N_sa, 1)\n",
        "        LOGT = np.log(S * A * N / delta)\n",
        "\n",
        "        # beta_r, beta_p = get(N_sa, S, A)\n",
        "        beta_r = np.sqrt(LOGT/N)\n",
        "        beta_p = np.sqrt(S*LOGT/N)\n",
        "        \n",
        "        # 2. run extended value iteration\n",
        "        V.fill(0)\n",
        "        for h in reversed(range(H)):\n",
        "            for s in range(S):\n",
        "                temp = np.zeros(A)\n",
        "                for a in range(A):\n",
        "                    dotp = LPprobaH(V[h + 1], Phat[s, a], beta_p[s, a])\n",
        "                    # V[h, s] = max_a  Rhat[s, a] + beta_r[s,a] + ...\n",
        "                    temp[a] = Rhat[s, a]+ beta_r[s,a] + dotp\n",
        "\n",
        "                # V[h, s] = np.max([temp[a_p] for a_p in range(A)])\n",
        "                V[h, s] = temp.max()\n",
        "                policy[h, s] = temp.argmax()\n",
        "        \n",
        "        # execute policy\n",
        "        initial_state = state = mdp.reset()\n",
        "        for h in range(H):\n",
        "            action = policy[h][state]\n",
        "            next_state, reward, done, _ = mdp.step(action)\n",
        "            \n",
        "            # update estimates (Phat, Rhat, N_sa, N_sas)\n",
        "\n",
        "            N_sa[state, action] += 1\n",
        "            N_sas[state, action, next_state] +=1\n",
        "            S_sa[state, action] += reward\n",
        "            \n",
        "            Rhat[state, action] = S_sa[state, action]/N_sa[state, action]\n",
        "            Phat[state, action, :] = N_sas[state, action, :]/N_sa[state, action]\n",
        "\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # update regret\n",
        "        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n",
        "        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n",
        "        \n",
        "        if k % 50 == 0:\n",
        "            print(\"regret[{}]: {}\".format(k, regret[k]))\n",
        "    return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-UTr3Gq7yK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fe068947-c81b-4d3e-e278-d25e532164d3"
      },
      "source": [
        "nb_repetitions = 5\n",
        "nb_episodes = 750\n",
        "regrets = np.zeros((nb_repetitions, nb_episodes))\n",
        "for it in range(nb_repetitions):\n",
        "    print(\"Running simulation: {}\".format(it))\n",
        "    regrets[it] = UCRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running simulation: 0\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3023839779999999\n",
            "regret[100]: 0.3400900322999999\n",
            "regret[150]: 0.3375917416249999\n",
            "regret[200]: 0.11446578837499993\n",
            "regret[250]: 0.12315131199999996\n",
            "regret[300]: 0.13974398349999995\n",
            "regret[350]: 0.11446578837499993\n",
            "regret[400]: 0.12315131199999996\n",
            "regret[450]: 0.05834768499999998\n",
            "regret[500]: 0.12315131199999996\n",
            "regret[550]: 0.09575785281249993\n",
            "regret[600]: 0.09575785281249993\n",
            "regret[650]: 0.09575785281249993\n",
            "regret[700]: 0.09575785281249993\n",
            "Running simulation: 1\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3023839779999999\n",
            "regret[100]: 0.3381416968749999\n",
            "regret[150]: 0.05919568499999994\n",
            "regret[200]: 0.038639677569999975\n",
            "regret[250]: 0.13338090439999997\n",
            "regret[300]: 0.062502475\n",
            "regret[350]: 0.09865332031249996\n",
            "regret[400]: 0.09713885281249995\n",
            "regret[450]: 0.09865332031249996\n",
            "regret[500]: 0.09713885281249995\n",
            "regret[550]: 0.09713885281249995\n",
            "regret[600]: 0.09660585281249995\n",
            "regret[650]: 0.05919568499999994\n",
            "regret[700]: 0.09575785281249993\n",
            "Running simulation: 2\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3023839779999999\n",
            "regret[100]: 0.3333550126937499\n",
            "regret[150]: 0.33896056331249985\n",
            "regret[200]: 0.038751103060000014\n",
            "regret[250]: 0.03513761854999997\n",
            "regret[300]: 0.10435752356249994\n",
            "regret[350]: 0.10466703881249992\n",
            "regret[400]: 0.09713885281249995\n",
            "regret[450]: 0.05972868499999995\n",
            "regret[500]: 0.09713885281249995\n",
            "regret[550]: 0.09660585281249995\n",
            "regret[600]: 0.09660585281249995\n",
            "regret[650]: 0.09660585281249995\n",
            "regret[700]: 0.05919568499999994\n",
            "Running simulation: 3\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3023839779999999\n",
            "regret[100]: 0.3300784179999999\n",
            "regret[150]: 0.3322523199374999\n",
            "regret[200]: 0.038639677569999975\n",
            "regret[250]: 0.10967521338249997\n",
            "regret[300]: 0.03513761854999997\n",
            "regret[350]: 0.03495190939999998\n",
            "regret[400]: 0.03363150300000001\n",
            "regret[450]: 0.031235826999999994\n",
            "regret[500]: 0.10227136281249993\n",
            "regret[550]: 0.031235826999999994\n",
            "regret[600]: 0.039653253312500025\n",
            "regret[650]: 0.05972868499999995\n",
            "regret[700]: 0.05972868499999995\n",
            "Running simulation: 4\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3023839779999999\n",
            "regret[100]: 0.3282411279999999\n",
            "regret[150]: 0.2362187972499999\n",
            "regret[200]: 0.003730034226250012\n",
            "regret[250]: 0.038639677569999975\n",
            "regret[300]: 0.03513761854999997\n",
            "regret[350]: 0.03495190939999998\n",
            "regret[400]: 0.03363150300000001\n",
            "regret[450]: 0.03363150300000001\n",
            "regret[500]: 0.031235826999999994\n",
            "regret[550]: 0.031235826999999994\n",
            "regret[600]: 0.039653253312500025\n",
            "regret[650]: 0.039653253312500025\n",
            "regret[700]: 0.039653253312500025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M49yfZ_S7yLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e55c12eb-ed8d-45fe-c462-dd8df0d72308"
      },
      "source": [
        "x = regrets.cumsum(axis=-1)\n",
        "mean_regret = x.mean(axis=0)\n",
        "std_regret = x.std(axis=0)\n",
        "plt.plot(mean_regret)\n",
        "plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n",
        "plt.ylabel('regret')\n",
        "\n",
        "# SAVE PSRL REGRET\n",
        "ucrl_regret = mean_regret"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+H++eiuToI2s7gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myPuG9Er7yLL",
        "colab_type": "text"
      },
      "source": [
        "# Posterior Sampling for RL\n",
        "\n",
        "At each iteration, PSRL samples one MDP from the posterior distribution and run the associated optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gxP6B9r7yLN",
        "colab_type": "text"
      },
      "source": [
        "Implement posterior sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl9YwtfJ7yLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PSRL(mdp, H, nb_episodes, VSTAR=0):\n",
        "    reward_prior = [1,1]\n",
        "    S, A = mdp.Ns, mdp.Na\n",
        "    Phat = np.ones((S,A,S)) / S\n",
        "    Rhat = np.zeros((S,A))\n",
        "    N_sas = np.zeros((S,A,S), dtype=np.int)\n",
        "    N_sa = np.zeros((S,A), dtype=np.int)\n",
        "    regret = np.zeros((nb_episodes,))\n",
        "    \n",
        "    for k in range(nb_episodes):\n",
        "        \n",
        "        # compute policy\n",
        "        # 1. sample MDP\n",
        "        R = np.zeros_like(Rhat)\n",
        "        P = np.zeros((S, A, S))\n",
        "        for s in range(S):\n",
        "            for a in range(A):\n",
        "                # sample transition matrix\n",
        "                # P[s, a] follows a dirichlet Dirichlet distribution of parameters N_sas[s, a,:] + 1\n",
        "                P[s, a] = ...\n",
        "\n",
        "                # posterior for Bernoulli rewards\n",
        "                N = N_sa[s, a]\n",
        "                v = N * Rhat[s, a]\n",
        "                a0 = reward_prior[0] + v\n",
        "                b0 = reward_prior[1] + N - v\n",
        "                p = np.random.beta(a=a0, b=b0, size=1).item()\n",
        "                R[s, a] = p\n",
        "        \n",
        "        # 2. compute optimal policy\n",
        "        V, policy = ...\n",
        "        \n",
        "        # execute policy\n",
        "        initial_state = state = mdp.reset()\n",
        "        for h in range(H):\n",
        "            action = policy[h][state]\n",
        "            next_state, reward, done, _ = mdp.step(action)\n",
        "            \n",
        "            # update estimates (Rhat, N_sa, N_sas)\n",
        "            ...\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # update regret\n",
        "        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n",
        "        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n",
        "        \n",
        "        if k % 50 == 0:\n",
        "            print(\"regret[{}]: {}\".format(k, regret[k]))\n",
        "\n",
        "    return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT38aVOB7yLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_repetitions = 5\n",
        "nb_episodes = 750\n",
        "regrets = np.zeros((nb_repetitions, nb_episodes))\n",
        "for it in range(nb_repetitions):\n",
        "    print(\"Running simulation: {}\".format(it))\n",
        "    regrets[it] = PSRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQhP7cpn7yLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = regrets.cumsum(axis=-1)\n",
        "mean_regret = x.mean(axis=0)\n",
        "std_regret = x.std(axis=0)\n",
        "plt.plot(mean_regret)\n",
        "plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n",
        "plt.ylabel('regret')\n",
        "\n",
        "# SAVE PSRL REGRET\n",
        "psrl_regret = mean_regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phHZWwdJ7yLc",
        "colab_type": "text"
      },
      "source": [
        "Compare algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvcqwaJY7yLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(ucrl_regret, label='UCRL-H')\n",
        "plt.plot(psrl_regret, label='PSRL')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it2tQtxL7yLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}